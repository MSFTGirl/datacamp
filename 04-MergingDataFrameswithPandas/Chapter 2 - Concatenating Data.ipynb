{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir('c:\\\\datacamp\\\\data\\\\')\n",
    "#Import Pittsburgh 2013 Weather CSV File\n",
    "weather = pd.read_csv('pittsburgh2013.csv', index_col=['Date'], parse_dates = True)\n",
    "\n",
    "#Separate the Mean Temp column and the Max Temp column\n",
    "pmeantemp = weather[['Mean TemperatureF']]\n",
    "pmaxtemp = weather[['Max TemperatureF']]\n",
    "\n",
    "#Year Variable needed for later exerises and dictionary creation\n",
    "year = ['Jan','Feb', 'Mar', 'Apr', 'May', 'June', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "#Find the mean temperature by month\n",
    "Janmean = pmeantemp.loc['2013-01'].max().iloc[0]\n",
    "Febmean = pmeantemp.loc['2013-02'].max().iloc[0]\n",
    "Marmean = pmeantemp.loc['2013-03'].max().iloc[0]\n",
    "Aprmean = pmeantemp.loc['2013-04'].max().iloc[0]\n",
    "Maymean = pmeantemp.loc['2013-05'].max().iloc[0]\n",
    "Junmean = pmeantemp.loc['2013-06'].max().iloc[0]\n",
    "Julmean = pmeantemp.loc['2013-07'].max().iloc[0]\n",
    "Augmean = pmeantemp.loc['2013-08'].max().iloc[0]\n",
    "Sepmean = pmeantemp.loc['2013-09'].max().iloc[0]\n",
    "Octmean = pmeantemp.loc['2013-10'].max().iloc[0]\n",
    "Novmean = pmeantemp.loc['2013-11'].max().iloc[0]\n",
    "Decmean = pmeantemp.loc['2013-12'].max().iloc[0]\n",
    "\n",
    "#Create a  dictionary and convert to dataframe of monthly mean temperatures\n",
    "mmeandict = {'Month':year,\n",
    "'Mean TemperatureF':[Janmean, Febmean, Marmean, Aprmean, Maymean, Junmean, Julmean, Augmean, Sepmean, Octmean, Novmean, Decmean]}\n",
    "weather_mean = pd.DataFrame(mmeandict).set_index('Month')\n",
    "\n",
    "#Find the max temperature by quarter\n",
    "Q1max = pmaxtemp.loc['2013-01-01':'2013-03-31'].max().iloc[0]\n",
    "Q2max = pmaxtemp.loc['2013-04-01':'2013-06-30'].max().iloc[0]\n",
    "Q3max = pmaxtemp.loc['2013-07-01':'2013-09-30'].max().iloc[0]\n",
    "Q4max = pmaxtemp.loc['2013-10-01':'2013-12-31'].max().iloc[0]\n",
    "\n",
    "#Create a  dictionary and convert to dataframe of quarterly max temperatures\n",
    "qmaxdict = {'Month':['Jan','Apr','Jul','Oct'],'Max TemperatureF':[Q1max,Q2max,Q3max,Q4max]}\n",
    "weather_max = pd.DataFrame(qmaxdict).set_index('Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging DataFrames with Pandas\n",
    "\n",
    "## Chapter 2 - Concatenating Data\n",
    "\n",
    "### Appending and Concatenating Series\n",
    "\n",
    "Combining DataFrames uses methods like .append() and the Pandas function, concat().\n",
    "\n",
    "#### .append()\n",
    "\n",
    "The .append() method will stack rows of Series and DataFrames. s1.append(s2) will stack rows from the Series s2 below the rows in the Series s1. \n",
    "\n",
    "#### concat()\n",
    "\n",
    "The Pandas function concat accepts a list or sequences of several DataFrames, pd.concat([s1, s2, s3]) and can concatenate by stacking row wise or column wise, depending on the options provided. When stacking multiple series, the concat() function is the equivalent to chaining multiple .append() methods:\n",
    "\n",
    "results1 = pd.concat([s1, s2, s3])\n",
    "results2 = s1.append(s2).append(s3)\n",
    "results1 == results2 element wise\n",
    "\n",
    "#### Series of US States\n",
    "\n",
    "By default, the 4 Series below of the US regions are indexed with integars, starting at 0. Using .append() on the northeast Series to append the south Series will result in a new series where the first 9 rows are the items from the northeast Series and the remaining rows are from the south Series. Notice that the .append() method stacks rows without adjusting the index values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     CT\n",
      "1     ME\n",
      "2     MA\n",
      "3     NH\n",
      "4     RI\n",
      "5     VT\n",
      "6     NJ\n",
      "7     NY\n",
      "8     PA\n",
      "0     DE\n",
      "1     FL\n",
      "2     GA\n",
      "3     MD\n",
      "4     NC\n",
      "5     SC\n",
      "6     VA\n",
      "7     DC\n",
      "8     WV\n",
      "9     AL\n",
      "10    KY\n",
      "11    MS\n",
      "12    TN\n",
      "13    AR\n",
      "14    LA\n",
      "15    OK\n",
      "16    TX\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "northeast = pd.Series(['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'])\n",
    "south = pd.Series(['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'])\n",
    "midwest = pd.Series(['IL', 'IN', 'MN', 'MO', 'NE', 'ND', 'SD', 'IA', 'KS', 'MI', 'OH', 'WI'])\n",
    "west = pd.Series(['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA'])\n",
    "\n",
    "east = northeast.append(south)\n",
    "print(east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Appended Index\n",
    "\n",
    "Notice that index for the east Series has duplicate values, which means when .loc() is called on say, index 3, two values are returned, one from the northeast Series and one from the south Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  0,  1,  2,  3,  4,  5,  6,  7,\n",
      "             8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(east.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    NH\n",
      "3    MD\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(east.loc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .reset_index()\n",
    "\n",
    "Having unique indexes is important and reset_index() method with the option drop=True will disgard the old indexes of each Series and creates a new index for the new Series. The new index is a type of RangeIndex with entries 0 to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     CT\n",
      "1     ME\n",
      "2     MA\n",
      "3     NH\n",
      "4     RI\n",
      "5     VT\n",
      "6     NJ\n",
      "7     NY\n",
      "8     PA\n",
      "9     DE\n",
      "10    FL\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "new_east = northeast.append(south).reset_index(drop=True)\n",
    "print(new_east.head(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=26, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(new_east.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using concat()\n",
    "\n",
    "The function concat() can construct an equivalent Series. Using a list of Series or DataFrames. The resulting index, as before, contains a list of repeated values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    CT\n",
      "1    ME\n",
      "2    MA\n",
      "3    NH\n",
      "4    RI\n",
      "5    VT\n",
      "6    NJ\n",
      "7    NY\n",
      "8    PA\n",
      "0    DE\n",
      "1    FL\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "east = pd.concat([northeast, south])\n",
    "print(east.head(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  0,  1,  2,  3,  4,  5,  6,  7,\n",
      "             8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(east.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ignore_index\n",
    "\n",
    "Similar to reset_index, the concat function has the ignore_index= option. When set to True, concat will reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     CT\n",
      "1     ME\n",
      "2     MA\n",
      "3     NH\n",
      "4     RI\n",
      "5     VT\n",
      "6     NJ\n",
      "7     NY\n",
      "8     PA\n",
      "9     DE\n",
      "10    FL\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "new_east=pd.concat([northeast, south], ignore_index = True)\n",
    "print(new_east.head(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "#### Appending pandas Series\n",
    "\n",
    "In this exercise, you'll load sales data from the months January, February, and March into DataFrames. Then, you'll extract Series with the 'Units' column from each and append them together with method chaining using .append().\n",
    "\n",
    "To check that the stacking worked, you'll print slices from these Series, and finally, you'll add the result to figure out the total units sold in the first quarter.\n",
    "\n",
    "__Instructions:__\n",
    "* Read the files 'sales-jan-2015.csv', 'sales-feb-2015.csv' and 'sales-mar-2015.csv' into the DataFrames jan, feb, and mar respectively.\n",
    "* Use parse_dates=True and index_col='Date'.\n",
    "* Extract the 'Units' column of jan, feb, and mar to create the Series jan_units, feb_units, and mar_units respectively.\n",
    "* Construct the Series quarter1 by appending feb_units to jan_units and then appending mar_units to the result. Use chained calls to the .append() method to do this.\n",
    "* Verify that quarter1 has the individual Series stacked vertically. To do this:\n",
    "* Print the slice containing rows from jan 27, 2015 to feb 2, 2015.\n",
    "* Print the slice containing rows from feb 26, 2015 to mar 7, 2015.\n",
    "* Compute and print the total number of units sold from the Series quarter1. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n",
      "642\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv('sales-jan-2015.csv', parse_dates = True, index_col='Date')\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv('sales-feb-2015.csv', parse_dates = True, index_col='Date')\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv('sales-mar-2015.csv', parse_dates = True, index_col='Date')\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas Series along row axis\n",
    "Having learned how to append Series, you'll now learn how to achieve the same result by concatenating Series instead. You'll continue to work with the sales data you've seen previously. This time, the DataFrames jan, feb, and mar have been pre-loaded.\n",
    "\n",
    "Your job is to use pd.concat() with a list of Series to achieve the same result that you would get by chaining calls to .append().\n",
    "\n",
    "You may be wondering about the difference between pd.concat() and pandas' .append() method. One way to think of the difference is that .append() is a specific case of a concatenation, while pd.concat() gives you more flexibility, as you'll see in later exercises.\n",
    "\n",
    "__Instructions:__\n",
    "* Create an empty list called units. This has been done for you.\n",
    "* Use a for loop to iterate over [jan, feb, mar]:\n",
    "* In each iteration of the loop, append the 'Units' column of each DataFrame to units.\n",
    "* Concatenate the Series contained in the list units into a longer Series called quarter1 using pd.concat().\n",
    "* Specify the keyword argument axis='rows' to stack the Series vertically.\n",
    "* Verify that quarter1 has the individual Series stacked vertically by printing slices. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month[\"Units\"])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units, axis='rows')\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending and Concatenating DataFrames\n",
    "\n",
    "A review of two 2010 population datasets shows that they same object types with the same shape, indexes and columns. When appended, these DataFrame are stacked row-wise, just like Series. Because both DataFrames have the same index, both .append() and concat will preserve the row indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (4, 1)\n",
      "<class 'pandas.core.frame.DataFrame'> (4, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pop1 = pd.read_csv('population_01.csv', index_col=0)\n",
    "pop2 = pd.read_csv('population_02.csv', index_col=0)\n",
    "print(type(pop1), pop1.shape)\n",
    "print(type(pop2), pop2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                2010 Census Population\n",
      "Zip Code ZCTA                         \n",
      "66407                              479\n",
      "72732                             4716\n",
      "50579                             2405\n",
      "46241                            30670\n",
      "                2010 Census Population\n",
      "Zip Code ZCTA                         \n",
      "12766                             2180\n",
      "76092                            26669\n",
      "98360                            12221\n",
      "49464                            27481\n"
     ]
    }
   ],
   "source": [
    "print(pop1)\n",
    "print(pop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010 Census Population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zip Code ZCTA</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66407</th>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72732</th>\n",
       "      <td>4716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50579</th>\n",
       "      <td>2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46241</th>\n",
       "      <td>30670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12766</th>\n",
       "      <td>2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76092</th>\n",
       "      <td>26669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98360</th>\n",
       "      <td>12221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49464</th>\n",
       "      <td>27481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                2010 Census Population\n",
       "Zip Code ZCTA                         \n",
       "66407                              479\n",
       "72732                             4716\n",
       "50579                             2405\n",
       "46241                            30670\n",
       "12766                             2180\n",
       "76092                            26669\n",
       "98360                            12221\n",
       "49464                            27481"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop1.append(pop2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrames with Different Indices\n",
    "\n",
    "The population DataFrame below has the same column names as the previous population DataFrames, but the unemployment DataFrame, has different shapes, column names and index. Notice that only zip code 2860 is a common index for both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                2010 Census Population\n",
      "Zip Code ZCTA                         \n",
      "57538                              322\n",
      "59916                              130\n",
      "37660                            40038\n",
      "2860                             45199\n",
      "        Unemployment   Participants\n",
      "Zip                                \n",
      "2860            0.11          34447\n",
      "46167           0.02           4800\n",
      "1097            0.33             42\n",
      "80808           0.07           4310\n"
     ]
    }
   ],
   "source": [
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "unemployment = pd.read_csv('unemployment.csv', index_col=0)\n",
    "print(population)\n",
    "print(unemployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the population and unemployment DataFrames are appended, the resulting DataFrame has 8 rows and 3 columns. The columns are the union of the columns from the input. The top 4 rows are from the population DataFrame and the .append() method has filled in Null values for columns that came over from the unemployment DataFrame and vice versa for the bottom 4 rows that are from the unemployment DataFrame.\n",
    "\n",
    "In addition, there are two rows with the same index number, 2860, one from the population DataFrame and one from the unemployment DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010 Census Population</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Participants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57538</th>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59916</th>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37660</th>\n",
       "      <td>40038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>45199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.11</td>\n",
       "      <td>34447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46167</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80808</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4310.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2010 Census Population   Unemployment   Participants\n",
       "57538                    322.0            NaN            NaN\n",
       "59916                    130.0            NaN            NaN\n",
       "37660                  40038.0            NaN            NaN\n",
       "2860                   45199.0            NaN            NaN\n",
       "2860                       NaN           0.11        34447.0\n",
       "46167                      NaN           0.02         4800.0\n",
       "1097                       NaN           0.33           42.0\n",
       "80808                      NaN           0.07         4310.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population.append(unemployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the population and unemployment DataFrames along the axis = 0 (aka axis = rows) will result in stacking rows vertically at the bottom; the result is identical to using the .append() method. The axis = 0 is the default action of .concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010 Census Population</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Participants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57538</th>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59916</th>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37660</th>\n",
       "      <td>40038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>45199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.11</td>\n",
       "      <td>34447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46167</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80808</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4310.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2010 Census Population   Unemployment   Participants\n",
       "57538                    322.0            NaN            NaN\n",
       "59916                    130.0            NaN            NaN\n",
       "37660                  40038.0            NaN            NaN\n",
       "2860                   45199.0            NaN            NaN\n",
       "2860                       NaN           0.11        34447.0\n",
       "46167                      NaN           0.02         4800.0\n",
       "1097                       NaN           0.33           42.0\n",
       "80808                      NaN           0.07         4310.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([population, unemployment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the axis = 1 option, also known as axis = columns, stacks the DataFrames horizontally, across the columns. This will result in a DataFrame with 7 rows and 3 columns, with the one common index, 2860, having all three columns filled in and the unique row having Null values inserted for columns from the other DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010 Census Population</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Participants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>45199.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>34447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37660</th>\n",
       "      <td>40038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46167</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57538</th>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59916</th>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80808</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4310.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2010 Census Population   Unemployment   Participants\n",
       "1097                       NaN           0.33           42.0\n",
       "2860                   45199.0           0.11        34447.0\n",
       "37660                  40038.0            NaN            NaN\n",
       "46167                      NaN           0.02         4800.0\n",
       "57538                    322.0            NaN            NaN\n",
       "59916                    130.0            NaN            NaN\n",
       "80808                      NaN           0.07         4310.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([population, unemployment], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "#### Appending DataFrames with ignore_index\n",
    "\n",
    "In this exercise, you'll use the Baby Names Dataset (from data.gov) again. This time, both DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, you'll specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information.\n",
    "\n",
    "__Instructions:__\n",
    "* Create a 'year' column in the DataFrames names_1881 and names_1981, with values of 1881 and 1981 respectively. Recall that assigning a scalar value to a DataFrame column broadcasts that value throughout.\n",
    "* Create a new DataFrame called combined_names by appending the rows of names_1981 underneath the rows of names_1881. Specify the keyword argument ignore_index=True to make a new RangeIndex of unique integers for each row.\n",
    "* Print the shapes of all three DataFrames. This has been done for you.\n",
    "* Extract all rows from combined_names that have the name 'Morgan'. To do this, use the .loc[] accessor with an appropriate filter. The relevant column of combined_names here is 'name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19455, 4)\n",
      "(1935, 4)\n",
      "(21390, 4)\n",
      "         Name Gender  Count  year\n",
      "1283   Morgan      M     23  1881\n",
      "2096   Morgan      F   1769  1981\n",
      "14390  Morgan      M    766  1981\n"
     ]
    }
   ],
   "source": [
    "names_1881 = pd.read_csv('names1881.csv', parse_dates=True, names=['Name', 'Gender','Count'])\n",
    "names_1981 = pd.read_csv('names1981.csv', parse_dates=True, names=['Name', 'Gender','Count'])\n",
    "\n",
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index = True)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names.loc[combined_names['Name']=='Morgan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Gender', 'Count', 'year'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_names.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>6919</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna</td>\n",
       "      <td>F</td>\n",
       "      <td>2698</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emma</td>\n",
       "      <td>F</td>\n",
       "      <td>2034</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>F</td>\n",
       "      <td>1852</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Margaret</td>\n",
       "      <td>F</td>\n",
       "      <td>1658</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>Wiliam</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>Wilton</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>Wing</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>Wood</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>Wright</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1935 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name Gender  Count  year\n",
       "0          Mary      F   6919  1881\n",
       "1          Anna      F   2698  1881\n",
       "2          Emma      F   2034  1881\n",
       "3     Elizabeth      F   1852  1881\n",
       "4      Margaret      F   1658  1881\n",
       "...         ...    ...    ...   ...\n",
       "1930     Wiliam      M      5  1881\n",
       "1931     Wilton      M      5  1881\n",
       "1932       Wing      M      5  1881\n",
       "1933       Wood      M      5  1881\n",
       "1934     Wright      M      5  1881\n",
       "\n",
       "[1935 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_1881"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas DataFrames along column axis\n",
    "The function pd.concat() can concatenate DataFrames horizontally as well as vertically (vertical is the default). To make the DataFrames stack horizontally, you have to specify the keyword argument axis=1 or axis='columns'.\n",
    "\n",
    "In this exercise, you'll use weather data with maximum and mean daily temperatures sampled at different rates (quarterly versus monthly). You'll concatenate the rows of both and see that, where rows are missing in the coarser DataFrame, null values are inserted in the concatenated DataFrame. This corresponds to an outer join (which you will explore in more detail in later exercises).\n",
    "\n",
    "The files 'quarterly_max_temp.csv' and 'monthly_mean_temp.csv' have been pre-loaded into the DataFrames weather_max and weather_mean respectively, and pandas has been imported as pd.\n",
    "\n",
    "__Instructions:__\n",
    "* Create weather_list, a list of the DataFrames weather_max and weather_mean.\n",
    "* Create a new DataFrame called weather by concatenating weather_list horizontally.\n",
    "* Pass the list to pd.concat() and specify the keyword argument axis=1 to stack them horizontally.\n",
    "* Print the new DataFrame weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Max TemperatureF  Mean TemperatureF\n",
      "Jan               68.0                 62\n",
      "Apr               89.0                 72\n",
      "Jul               91.0                 80\n",
      "Oct               84.0                 74\n",
      "Feb                NaN                 48\n",
      "Mar                NaN                 55\n",
      "May                NaN                 77\n",
      "June               NaN                 78\n",
      "Aug                NaN                 77\n",
      "Sep                NaN                 79\n",
      "Nov                NaN                 60\n",
      "Dec                NaN                 62\n"
     ]
    }
   ],
   "source": [
    "# Create a list of weather_max and weather_mean\n",
    "weather_list = [weather_max, weather_mean]\n",
    "\n",
    "# Concatenate weather_list horizontally\n",
    "weather = pd.concat(weather_list, axis=1)\n",
    "\n",
    "# Print weather\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading multiple files to build a DataFrame\n",
    "It is often convenient to build a large DataFrame by parsing many files as DataFrames and concatenating them all at once. You'll do this here with three files, but, in principle, this approach can be used to combine data from dozens or hundreds of files.\n",
    "\n",
    "Here, you'll work with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "pandas has been imported as pd and the list medal_types has been pre-loaded for you, which contains the strings 'bronze', 'silver', and 'gold'.\n",
    "\n",
    "__Instructions:__\n",
    "* Iterate over medal_types in the for loop.\n",
    "* Inside the for loop:\n",
    "  * Create file_name using string interpolation with the loop variable medal. This has been done for you. The expression \"%s_top5.csv\" % medal evaluates as a string with the value of medal replacing %s in the format string.\n",
    "  * Create the list of column names called columns. This has been done for you.\n",
    "  * Read file_name into a DataFrame called medal_df. Specify the keyword arguments header=0, index_col='Country', and names=columns to get the correct row and column Indexes.\n",
    "  * Append medal_df to medals using the list .append() method.\n",
    "* Concatenate the list of DataFrames medals horizontally (using axis='columns') to create a single DataFrame called medals_df. Print it in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "United States   1052.0  1195.0  2088.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n",
      "France           475.0   461.0     NaN\n",
      "Germany          454.0     NaN   407.0\n",
      "Italy              NaN   394.0   460.0\n"
     ]
    }
   ],
   "source": [
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "#Initialize an empyy list: medals\n",
    "medals =[]\n",
    "\n",
    "for medal in medal_types:\n",
    "    # Create the file name: file_name\n",
    "    file_name = '%s_top5.csv' % medal\n",
    "    # Create list of column names: columns\n",
    "    columns = ['Country', medal]\n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, header = 0, index_col='Country', names=columns)\n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "# Concatenate medals horizontally: medals_df\n",
    "medals_df = pd.concat(medals, axis='columns')\n",
    "\n",
    "# Print medals_df\n",
    "print(medals_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation, Keys and MultiIndexes\n",
    "\n",
    "Examining the rain2013 and rain2014 DataFrames, we see there are common row levels and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain 2013 Q1\n",
      "        Precipitation\n",
      "Month                \n",
      "Jan          0.096129\n",
      "Feb          0.067143\n",
      "Mar          0.061613\n",
      "Rain 2014 Q1\n",
      "        Precipitation\n",
      "Month                \n",
      "Jan          0.050323\n",
      "Feb          0.082143\n",
      "Mar          0.070968\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file1 = 'rainQ12013.csv'\n",
    "file2 = 'rainQ12014.csv'\n",
    "\n",
    "rain2013 = pd.read_csv(file1, index_col = 'Month', parse_dates = True)\n",
    "rain2014 = pd.read_csv(file2, index_col = 'Month', parse_dates = True)\n",
    "\n",
    "print(\"Rain 2013 Q1\")\n",
    "print(rain2013)\n",
    "print('Rain 2014 Q1')\n",
    "print(rain2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using concat with the default of axis = 0 will create a single DataFrame with recurring indices and obscures the fact that the bottom 3 rows are from the 2014 DataFrame while the top 3 are from the 2013 DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precipitation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jan</th>\n",
       "      <td>0.096129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feb</th>\n",
       "      <td>0.067143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mar</th>\n",
       "      <td>0.061613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jan</th>\n",
       "      <td>0.050323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feb</th>\n",
       "      <td>0.082143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mar</th>\n",
       "      <td>0.070968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Precipitation\n",
       "Month                \n",
       "Jan          0.096129\n",
       "Feb          0.067143\n",
       "Mar          0.061613\n",
       "Jan          0.050323\n",
       "Feb          0.082143\n",
       "Mar          0.070968"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([rain2013, rain2014])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Multi-index on Rows\n",
    "\n",
    "A way to address the issue above is to use a multi-level index on the rows by passing the keys = option on concat() a list of outer index labels. This assigns an outer index label for each of the original input data. The order of the list of keys must match the order of the list of input DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Precipitation\n",
      "     Month                \n",
      "2013 Jan          0.096129\n",
      "     Feb          0.067143\n",
      "     Mar          0.061613\n",
      "2014 Jan          0.050323\n",
      "     Feb          0.082143\n",
      "     Mar          0.070968\n"
     ]
    }
   ],
   "source": [
    "rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis = 0)\n",
    "print(rain1314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a Multi-index\n",
    "\n",
    "As expected, the outer most index can be selected when slicing the combined DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precipitation\n",
      "Month                \n",
      "Jan          0.050323\n",
      "Feb          0.082143\n",
      "Mar          0.070968\n"
     ]
    }
   ],
   "source": [
    "print(rain1314.loc[2014])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating Columns\n",
    "\n",
    "A different approach would be the add the 2014 data across the columns to the 2013 data. This is done using axis = 1 or axis = columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precipitation   Precipitation\n",
      "Month                                \n",
      "Jan          0.096129        0.050323\n",
      "Feb          0.067143        0.082143\n",
      "Mar          0.061613        0.070968\n"
     ]
    }
   ],
   "source": [
    "rain1314 = pd.concat([rain2013, rain2014], axis = 'columns')\n",
    "print(rain1314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that the resulting DataFrame obscures which Precipitation column comes from which year. This is addressed using the keys = option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                2013           2014\n",
      "       Precipitation  Precipitation\n",
      "Month                              \n",
      "Jan         0.096129       0.050323\n",
      "Feb         0.067143       0.082143\n",
      "Mar         0.061613       0.070968\n"
     ]
    }
   ],
   "source": [
    "rain1314 = pd.concat([rain2013, rain2014], keys = [2013, 2014], axis = 'columns')\n",
    "print(rain1314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat() with Dict\n",
    "\n",
    "Finally, the concat() function can accept a dictionary rather than a list of DataFrames. In this scenario, the dictionary keys are automatically treated as values for the keys = argument when building a multi-index on the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                2013           2014\n",
      "       Precipitation  Precipitation\n",
      "Month                              \n",
      "Jan         0.096129       0.050323\n",
      "Feb         0.067143       0.082143\n",
      "Mar         0.061613       0.070968\n"
     ]
    }
   ],
   "source": [
    "rain_dict = {2013: rain2013, 2014: rain2014}\n",
    "rain1314 = pd.concat(rain_dict, axis = 'columns')\n",
    "print(rain1314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "#### Concatenating vertically to get MultiIndexed rows\n",
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the keys parameter in the call to pd.concat(), which generates a hierarchical index with the labels from keys as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified.\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset. Once again, pandas has been imported as pd and two lists have been pre-loaded: An empty list called medals, and medal_types, which contains the strings 'bronze', 'silver', and 'gold'.\n",
    "\n",
    "__Instructions:__\n",
    "* Within the for loop:\n",
    " * Read file_name into a DataFrame called medal_df. Specify the index to be 'Country'.\n",
    " * Append medal_df to medals.\n",
    "* Concatenate the list of DataFrames medals into a single DataFrame called medals. Be sure to use the keyword argument keys=['bronze', 'silver', 'gold'] to create a vertically stacked DataFrame with a MultiIndex.\n",
    "* Print the new DataFrame medals. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Total\n",
      "       Country               \n",
      "bronze United States   1052.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       France           475.0\n",
      "       Germany          454.0\n",
      "silver United States   1195.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       France           461.0\n",
      "       Italy            394.0\n",
      "gold   United States   2088.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       Italy            460.0\n",
      "       Germany          407.0\n"
     ]
    }
   ],
   "source": [
    "medals =[]\n",
    "for medal in medal_types:\n",
    "\n",
    "    file_name = '%s_top5.csv' % medal\n",
    "    \n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col='Country')\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "    \n",
    "# Concatenate medals: medals\n",
    "medals = pd.concat(medals, keys=['bronze', 'silver', 'gold'])\n",
    "\n",
    "# Print medals in entirety\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Total\n",
      "       Country               \n",
      "bronze United States   1052.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       France           475.0\n",
      "       Germany          454.0\n",
      "silver United States   1195.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       France           461.0\n",
      "       Italy            394.0\n",
      "gold   United States   2088.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       Italy            460.0\n",
      "       Germany          407.0\n"
     ]
    }
   ],
   "source": [
    "medals = []\n",
    "for medal in medal_types:\n",
    "\n",
    "    file_name = '%s_top5.csv' % medal\n",
    "    \n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col='Country')\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "    # Concatenate medals: medals\n",
    "medals = pd.concat(medals, keys=['bronze', 'silver', 'gold'])\n",
    "\n",
    "# Print medals in entirety\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing MultiIndexed DataFrames\n",
    "This exercise picks up where the last ended (again using The Guardian's Olympic medal dataset).\n",
    "\n",
    "You are provided with the MultiIndexed DataFrame as produced at the end of the preceding exercise. Your task is to sort the DataFrame and to use the pd.IndexSlice to extract specific slices. Check out this exercise from Manipulating DataFrames with pandas to refresh your memory on how to deal with MultiIndexed DataFrames.\n",
    "\n",
    "pandas has been imported for you as pd and the DataFrame medals is already in your namespace.\n",
    "\n",
    "__Instructions:__\n",
    " * Create a new DataFrame medals_sorted with the entries of medals sorted. Use .sort_index(level=0) to ensure the Index is sorted suitably.\n",
    " * Print the number of bronze medals won by Germany and all of the silver medal data. This has been done for you.\n",
    " * Create an alias for pd.IndexSlice called idx. A slicer pd.IndexSlice is required when slicing on the inner level of a MultiIndex.\n",
    " * Slice all the data on medals won by the United Kingdom in the DataFrame medals_sorted. To do this, use the .loc[] accessor with <br>\n",
    " idx[:,'United Kingdom'], :."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Behind the scene data preparation for the exercise\n",
    "\n",
    "gold = pd.read_csv('gold_top5.csv', index_col = 'Country')\n",
    "silver = pd.read_csv('silver_top5.csv', index_col = 'Country')\n",
    "bronze =pd.read_csv('bronze_top5.csv', index_col = 'Country')\n",
    "\n",
    "medals = pd.concat([gold, silver, bronze], axis = 0, keys = ['gold', 'silver','bronze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total    454\n",
      "Name: (bronze, Germany), dtype: int64\n",
      "                Total\n",
      "Country              \n",
      "France            461\n",
      "Italy             394\n",
      "Soviet Union      627\n",
      "United Kingdom    591\n",
      "United States    1195\n",
      "                       Total\n",
      "       Country              \n",
      "bronze United Kingdom    505\n",
      "gold   United Kingdom    498\n",
      "silver United Kingdom    591\n"
     ]
    }
   ],
   "source": [
    "# Sort the entries of medals: medals_sorted\n",
    "medals_sorted = medals.sort_index(level=0)\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:,'United Kingdom'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating horizontally to get MultiIndexed columns\n",
    "It is also possible to construct a DataFrame with hierarchically indexed columns. For this exercise, you'll start with pandas imported and a list of three DataFrames called dataframes. All three DataFrames contain 'Company', 'Product', and 'Units' columns with a 'Date' column as the index pertaining to sales transactions during the month of February, 2015. The first DataFrame describes Hardware transactions, the second describes Software transactions, and the third, Service transactions.\n",
    "\n",
    "Your task is to concatenate the DataFrames horizontally and to create a MultiIndex on the columns. From there, you can summarize the resulting DataFrame and slice some information from it.\n",
    "\n",
    "__Instructions:__\n",
    "* Construct a new DataFrame february with MultiIndexed columns by concatenating the list dataframes.\n",
    "* Use axis=1 to stack the DataFrames horizontally and the keyword argument keys=['Hardware', 'Software', 'Service'] to construct a hierarchical Index from each DataFrame.\n",
    "* Print summary information from the new DataFrame february using the .info() method. This has been done for you.\n",
    "* Create an alias called idx for pd.IndexSlice.\n",
    "* Extract a slice called slice_2_8 from february (using .loc[] & idx) that comprises rows between Feb. 2, 2015 to Feb. 8, 2015 from columns under 'Company'.\n",
    "* Print the slice_2_8. This has been done for you, so hit 'Submit Answer' to see the sliced data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Behind the scene data preparation for the exercise\n",
    "\n",
    "jan = pd.read_csv('sales-jan-2015.csv', parse_dates = True, index_col = 'Date')\n",
    "feb = pd.read_csv('sales-feb-2015.csv', parse_dates = True, index_col = 'Date')\n",
    "mar = pd.read_csv('sales-mar-2015.csv', parse_dates = True, index_col = 'Date')\n",
    "\n",
    "\n",
    "dataframes = [jan[jan['Product'] == 'Hardware'].append(feb[feb['Product']=='Hardware']).append(mar[mar['Product']=='Hardware']),\n",
    "              jan[jan['Product'] == 'Software'].append(feb[feb['Product']=='Software']).append(mar[mar['Product']=='Software']),\n",
    "              jan[jan['Product'] == 'Service'].append(feb[feb['Product']=='Service']).append(mar[mar['Product']=='Service'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 60 entries, 2015-01-01 07:31:00 to 2015-03-28 19:20:00\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   (Hardware, Company)  21 non-null     object \n",
      " 1   (Hardware, Product)  21 non-null     object \n",
      " 2   (Hardware, Units)    21 non-null     float64\n",
      " 3   (Software, Company)  23 non-null     object \n",
      " 4   (Software, Product)  23 non-null     object \n",
      " 5   (Software, Units)    23 non-null     float64\n",
      " 6   (Service, Company)   16 non-null     object \n",
      " 7   (Service, Product)   16 non-null     object \n",
      " 8   (Service, Units)     16 non-null     float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 4.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "february = pd.concat(dataframes, axis = 1, keys=['Hardware', 'Software', 'Service'])\n",
    "print(february.info())\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['Feb.2, 2015':'Feb.8,2015', idx[:, 'Company']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 53 entries, 2015-01-01 07:31:00 to 2015-03-28 19:20:00\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   (Hardware, Company)  21 non-null     object \n",
      " 1   (Hardware, Product)  21 non-null     object \n",
      " 2   (Hardware, Units)    21 non-null     float64\n",
      " 3   (Software, Company)  22 non-null     object \n",
      " 4   (Software, Product)  22 non-null     object \n",
      " 5   (Software, Units)    22 non-null     float64\n",
      " 6   (Service, Company)   10 non-null     object \n",
      " 7   (Service, Product)   10 non-null     object \n",
      " 8   (Service, Units)     10 non-null     float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 4.1+ KB\n",
      "None\n",
      "                             Hardware          Software Service\n",
      "                              Company           Company Company\n",
      "Date                                                           \n",
      "2015-02-04 15:36:00               NaN         Streeplex     NaN\n",
      "2015-02-04 21:52:00  Acme Corporation               NaN     NaN\n",
      "2015-02-05 01:53:00               NaN  Acme Corporation     NaN\n",
      "2015-02-07 22:58:00  Acme Corporation               NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, axis = 1, keys=['Hardware', 'Software', 'Service'])\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['Feb.4, 2015':'Feb.8,2015', idx[:, 'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating DataFrames from a dict\n",
    "You're now going to revisit the sales data you worked with earlier in the chapter. Three DataFrames jan, feb, and mar have been pre-loaded for you. Your task is to aggregate the sum of all sales over the 'Company' column into a single DataFrame. You'll do this by constructing a dictionary of these DataFrames and then concatenating them.\n",
    "\n",
    "__Instructions:__\n",
    "* Create a list called month_list consisting of the tuples ('january', jan), ('february', feb), and ('march', mar).\n",
    "* Create an empty dictionary called month_dict.\n",
    "* Inside the for loop:\n",
    " * Group month_data by 'Company' and use .sum() to aggregate.\n",
    "* Construct a new DataFrame called sales by concatenating the DataFrames stored in month_dict.\n",
    "* Create an alias for pd.IndexSlice and print all sales by 'Mediacore'. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Units\n",
      "         Company                \n",
      "january  Acme Coporation      76\n",
      "         Hooli                70\n",
      "         Initech              37\n",
      "         Mediacore            15\n",
      "         Streeplex            50\n",
      "february Acme Corporation     34\n",
      "         Hooli                30\n",
      "         Initech              13\n",
      "         Initech Service      10\n",
      "         InitechSoftware       7\n",
      "         Mediacore            45\n",
      "         Streeplex            37\n",
      "march    Acme Corporation      5\n",
      "         Hooli                37\n",
      "         Initech              68\n",
      "         Mediacore            68\n",
      "         Streeplex            40\n",
      "                    Units\n",
      "         Company         \n",
      "january  Mediacore     15\n",
      "february Mediacore     45\n",
      "march    Mediacore     68\n"
     ]
    }
   ],
   "source": [
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out and Inner Joins\n",
    "\n",
    "This lesson is to help better understand what append and concatenate are doing when joining DataFrames and Series together. \n",
    "\n",
    "Let's start with stacking arrays. First we create 3 arrays, A, B and C: sized 2 by 4, 2 by 3 and 3 by 4 respectively. The + constant added to the end is to allow us to visualize which array the numbers came from as we proceed to join these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 1.1 2.1 3.1]\n",
      " [4.1 5.1 6.1 7.1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "A = np.arange(8).reshape(2,4) + .1\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2 1.2 2.2]\n",
      " [3.2 4.2 5.2]]\n"
     ]
    }
   ],
   "source": [
    "B = np.arange(6).reshape(2,3) + .2\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3  1.3  2.3  3.3]\n",
      " [ 4.3  5.3  6.3  7.3]\n",
      " [ 8.3  9.3 10.3 11.3]]\n"
     ]
    }
   ],
   "source": [
    "C = np.arange(12).reshape(3,4) + .3\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Arrays Horizontally\n",
    "\n",
    "We can stack the 2x4 matrix A with the 2x3 matrix B horizontally using np.hstack(). Equivalently, np.concatenate, with the axis = 1 option will create the same array. In both cases, A and B must have the same number of rows, but the number of columns can differ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 1.2, 2.2, 0.1, 1.1, 2.1, 3.1],\n",
       "       [3.2, 4.2, 5.2, 4.1, 5.1, 6.1, 7.1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([B,A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 1.2, 2.2, 0.1, 1.1, 2.1, 3.1],\n",
       "       [3.2, 4.2, 5.2, 4.1, 5.1, 6.1, 7.1]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([B, A], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Arrays Vertically\n",
    "\n",
    "We can also stack the 2x4 matrix A and the 3x4 matrix C vertically using np.vstack() or np.concatenate() with axis = 0, the default for np.concatenate(). This case it's important that both matrices have 4 columns, but the number of rows can differ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  1.1,  2.1,  3.1],\n",
       "       [ 4.1,  5.1,  6.1,  7.1],\n",
       "       [ 0.3,  1.3,  2.3,  3.3],\n",
       "       [ 4.3,  5.3,  6.3,  7.3],\n",
       "       [ 8.3,  9.3, 10.3, 11.3]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([A,C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  1.1,  2.1,  3.1],\n",
       "       [ 4.1,  5.1,  6.1,  7.1],\n",
       "       [ 0.3,  1.3,  2.3,  3.3],\n",
       "       [ 4.3,  5.3,  6.3,  7.3],\n",
       "       [ 8.3,  9.3, 10.3, 11.3]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([A,C], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value error Exception is raised when you try to concatenate arrays of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-09b3d4950289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3"
     ]
    }
   ],
   "source": [
    "np.concatenate([A,B], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2 and the array at index 1 has size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-579946578552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2 and the array at index 1 has size 3"
     ]
    }
   ],
   "source": [
    "np.concatenate([A,C], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the population and unemployment data, we see that population has 4 rows and 2 columns and the unemployment data has 4 rows and 3 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                2010 Census Population\n",
      "Zip Code ZCTA                         \n",
      "57538                              322\n",
      "59916                              130\n",
      "37660                            40038\n",
      "2860                             45199\n",
      "        Unemployment   Participants\n",
      "Zip                                \n",
      "2860            0.11          34447\n",
      "46167           0.02           4800\n",
      "1097            0.33             42\n",
      "80808           0.07           4310\n"
     ]
    }
   ],
   "source": [
    "print(population)\n",
    "print(unemployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to Arrays\n",
    "\n",
    "Recall that Pandas is based on NumPy, so first let's convert the above DataFrames to NumPy arrays. Notice that with the array, the index information is disgarded. And we know have 2 NumPy arrays, dimensions 4x1 and 4x2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  322]\n",
      " [  130]\n",
      " [40038]\n",
      " [45199]]\n"
     ]
    }
   ],
   "source": [
    "population_array = np.array(population)\n",
    "print(population_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1000e-01 3.4447e+04]\n",
      " [2.0000e-02 4.8000e+03]\n",
      " [3.3000e-01 4.2000e+01]\n",
      " [7.0000e-02 4.3100e+03]]\n"
     ]
    }
   ],
   "source": [
    "unemployment_array = np.array(unemployment)\n",
    "print(unemployment_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating Data as Arrays\n",
    "\n",
    "We could use apply np.concatenate() or np.hstack() to these arrays, but the new array, dimension 4x3, would be meaningless. The rows are simply glued together, independent of the original meaning. It would be necessary to store the zip codes in other lists or arrays to align rows properly. Appending label tables in this manner corresponds to what is called a database join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2200e+02, 1.1000e-01, 3.4447e+04],\n",
       "       [1.3000e+02, 2.0000e-02, 4.8000e+03],\n",
       "       [4.0038e+04, 3.3000e-01, 4.2000e+01],\n",
       "       [4.5199e+04, 7.0000e-02, 4.3100e+03]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([population_array, unemployment_array], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins\n",
    "\n",
    "Joining tables involves meaningfully gluing index rows together. An outer join preserves the indices in the original table, filling Null values for missing rows. An outer joined table has all the indices of the original tables without any repetition, like a set union. Conversely, an inner join only has index labels common to both tables, like a set intersection. \n",
    "\n",
    "#### Concatenation and Inner Join\n",
    "If we call concat and specify the option axis = to 1, or columns, and the option join= to 'inner' for an inner join which will result in only 1 row where the intersection of both DataFrames occurs, zip code 2860."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010 Census Population</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Participants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>45199</td>\n",
       "      <td>0.11</td>\n",
       "      <td>34447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       2010 Census Population   Unemployment   Participants\n",
       "2860                    45199           0.11          34447"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([population, unemployment], axis = 1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation and Outer Join\n",
    "if we specify join = 'outer' across axis = 1, we get the default behavior of the .concat() function where the unspecified joined paramete defaults to Null. When a row entry occurs in one DataFrame and not the other, the missing entries are filled the a Null value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010 Census Population</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Participants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>45199.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>34447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37660</th>\n",
       "      <td>40038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46167</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57538</th>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59916</th>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80808</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4310.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2010 Census Population   Unemployment   Participants\n",
       "1097                       NaN           0.33           42.0\n",
       "2860                   45199.0           0.11        34447.0\n",
       "37660                  40038.0            NaN            NaN\n",
       "46167                      NaN           0.02         4800.0\n",
       "57538                    322.0            NaN            NaN\n",
       "59916                    130.0            NaN            NaN\n",
       "80808                      NaN           0.07         4310.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([population, unemployment], axis = 1, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Join on the Other Axis\n",
    "\n",
    "We could also do an inner join on the axis = 0. No column index label occurs in both population and unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of Empty DataFrame\n",
       "Columns: []\n",
       "Index: [57538, 59916, 37660, 2860, 2860, 46167, 1097, 80808]>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([population, unemployment], join = 'inner', axis = 0).info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "#### Concatenating DataFrames with inner join\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "The DataFrames bronze, silver, and gold have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join.\n",
    "\n",
    "__Instructions:__\n",
    "* Construct a list of DataFrames called medal_list with entries bronze, silver, and gold.\n",
    "* Concatenate medal_list horizontally with an inner join to create medals.\n",
    "* Use the keyword argument keys=['bronze', 'silver', 'gold'] to yield suitable hierarchical indexing.\n",
    "* Use axis=1 to get horizontal concatenation.\n",
    "* Use join='inner' to keep only rows that share common index labels.\n",
    "* Print the new DataFrame medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               bronze silver  gold\n",
      "                Total  Total Total\n",
      "Country                           \n",
      "United States    1052   1195  2088\n",
      "Soviet Union      584    627   838\n",
      "United Kingdom    505    591   498\n"
     ]
    }
   ],
   "source": [
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze,silver,gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, axis = 1, join = 'inner', keys = ['bronze', 'silver', 'gold'])\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! France, Italy, and Germany got dropped as part of the join since they are not present in each of bronze, silver, and gold. Therefore, the final DataFrame has only the United States, Soviet Union, and United Kingdom.\n",
    "\n",
    "#### Resampling & concatenating DataFrames with inner join\n",
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1961 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate offset alias for resampling, and the method .resample() must be chained with some kind of aggregation method (.pct_change() and .last() in this case).\n",
    "\n",
    "pandas has been imported as pd, and the DataFrames china and us have been pre-loaded, with the output of china.head() and us.head() printed in the IPython Shell.\n",
    "\n",
    "__Instructions:__\n",
    "* Make a new DataFrame china_annual by resampling the DataFrame china with .resample('A').last() (i.e., with annual frequency) and chaining two method calls:\n",
    "* Chain .pct_change(10) as an aggregation method to compute the percentage change with an offset of ten years.\n",
    "* Chain .dropna() to eliminate rows containing null values.\n",
    "* Make a new DataFrame us_annual by resampling the DataFrame us exactly as you resampled china.\n",
    "* Concatenate china_annual and us_annual to construct a DataFrame called gdp. Use join='inner' to perform an inner join and use axis=1 to concatenate horizontally.\n",
    "* Print the result of resampling gdp every decade (i.e., using .resample('10A')) and aggregating with the method .last(). This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 GDP     Value\n",
      "Year                          \n",
      "1970-12-31  0.546128  1.017187\n",
      "1980-12-31  1.072537  1.742556\n",
      "1990-12-31  0.892820  1.012126\n",
      "2000-12-31  2.357522  0.738632\n",
      "2010-12-31  4.011081  0.454332\n",
      "2020-12-31  3.789936  0.361780\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Behind the scenes work to load data that was already loaded in DataCamp\n",
    "china = pd.read_csv('gdp_china.csv')\n",
    "us = pd.read_csv('gdp_usa.csv')\n",
    "china['Year'] = pd.to_datetime(china['Year'])\n",
    "us['Year'] = pd.to_datetime(us['Year'])\n",
    "us = us.set_index('Year')\n",
    "china = china.set_index('Year')\n",
    "\n",
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').last().pct_change(10).dropna()\n",
    "\n",
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').last().pct_change(10).dropna()\n",
    "\n",
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual, us_annual], join='inner', axis=1)\n",
    "\n",
    "# Resample gdp and print\n",
    "print(gdp.resample('10A').last())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
