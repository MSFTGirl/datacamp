{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data in Python (Part 2)\n",
    "\n",
    "## Chapter 1 - Importing Data from the Internet\n",
    "\n",
    "### Importing Flat Files from the Web\n",
    "\n",
    "Importing data from the web should be automated in a Python file to ensure that code is both reproducible and scalable. Manually going to a URL and clicking the download data file link is neither. Tools like HTTP GET request, scraping data from HTML and using the urllib and request package are all ways to automate retrieving data from the Internet. \n",
    "\n",
    "#### Urllib Package\n",
    "The urllib package provides a high level interface for fetching data across the web. urllib has a urlopen() function that is similar to the built-in Python open function, but urlopen() accepts URL addresses rather than file names. Here is an example of importing the white wine data set from the UC Irvine Wine Quality web site. When calling the urlretrieve() function, you pass both the URL to fetch the data from, as well as the file name you want the function to store the data as. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('winequality-white.csv', <http.client.HTTPMessage at 0x15ba0748130>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('c:\\\\datacamp\\\\data\\\\')\n",
    "from urllib.request import urlretrieve\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "urlretrieve(url, 'winequality-white.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "#### Importing flat files from the web: your turn!\n",
    "You are about to import your first file from the web! The flat file you will import will be 'winequality-red.csv' from the University of California, Irvine's Machine Learning repository. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.\n",
    "\n",
    "The URL of the file is\n",
    "> 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'<br>\n",
    "\n",
    "After you import it, you'll check your working directory to confirm that it is there and then you'll load it into a pandas DataFrame.\n",
    "\n",
    "__Instructions:__\n",
    "* Import the function urlretrieve from the subpackage urllib.request.\n",
    "* Assign the URL of the file to the variable url.\n",
    "* Use the function urlretrieve() to save the file locally as 'winequality-red.csv'.\n",
    "* Execute the remaining code to load 'winequality-red.csv' in a pandas DataFrame and to print its head to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Opening and reading flat files from the web\n",
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load \n",
    "a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas. In particular, \n",
    "you can use the function pd.read_csv() with the URL as the first argument and the separator sep as the second argument.\n",
    "\n",
    "Instructions\n",
    "Assign the URL of the file to the variable url.\n",
    "Read file into a DataFrame df using pd.read_csv(), recalling that the separator in the file is ';'.\n",
    "Print the head of the DataFrame df.\n",
    "Execute the rest of the code to plot histogram of the first feature in the DataFrame df\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "df = pd.read_csv(url, delimiter = ';')\n",
    "print(df.head())\n",
    "\n",
    "pd.DataFrame.hist(df.iloc[:, 0:1])\n",
    "plt.xlabel('fixed acidity(g(tartaric acid)/dm$^*3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening and reading flat files from the web\n",
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas. In particular, you can use the function pd.read_csv() with the URL as the first argument and the separator sep as the second argument.\n",
    "\n",
    "The URL of the file, once again, is\n",
    "> 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'<br>\n",
    "\n",
    "__Instructions:__\n",
    "* Assign the URL of the file to the variable url.\n",
    "* Read file into a DataFrame df using pd.read_csv(), recalling that the separator in the file is ';'.\n",
    "* Print the head of the DataFrame df.\n",
    "* Execute the rest of the code to plot histogram of the first feature in the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, delimiter = ';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "pd.DataFrame.hist(df.iloc[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing non-flat files from the web\n",
    "Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the pandas function pd.read_csv(). This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use pd.read_excel() to import an Excel spreadsheet.\n",
    "\n",
    "The URL of the spreadsheet is:\n",
    "> 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'<br>\n",
    "\n",
    "Your job is to use pd.read_excel() to read in all of its sheets, print the sheet names and then print the head of the first sheet using its name, not its index.\n",
    "\n",
    "Note that the output of pd.read_excel() is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.\n",
    "\n",
    "__Instructions:__\n",
    "* Assign the URL of the file to the variable url.\n",
    "* Read the file in url into a dictionary xls using pd.read_excel() recalling that, in order to import all sheets you need to pass None to the argument sheet_name.\n",
    "* Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary xls.\n",
    "* Print the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is '1700'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1700', '1900'])\n",
      "                 country       1700\n",
      "0            Afghanistan  34.565000\n",
      "1  Akrotiri and Dhekelia  34.616667\n",
      "2                Albania  41.312000\n",
      "3                Algeria  36.720000\n",
      "4         American Samoa -14.307000\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xl\n",
    "xl = pd.read_excel(url, sheet_name = None)\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xl.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xl['1700'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Requests To Import Files From The Web\n",
    "\n",
    "#### URL\n",
    "An acronym that stands for Universal Resource Locator, or a reference to web resources. The vast majority of URLs are web addresses, but it can also refer to File Transfer Protocols (FTP) and database access. For web addresses, the URL contains two parts, the protocol identifier - http: or https: and a resource name, like datacamp.com. Together the protocol indentifier and the resource name create a web address.\n",
    "\n",
    "#### HTTP\n",
    "The protocol identifier typically used, HTTP, stands for HyperText Transfer Protocol. HTTP is the foundation of data communications for the world wide web. HTTPS is a more secure version of HTTP. Everytime you go to a web site, your browser is sending an HTTP GET request. urlretrieve() peforms a GET request and then saves the relevant data locally. \n",
    "\n",
    "In particular, you will want to use urlretrieve() to get HTML data from a web page. HTML stands for HyperText Markup Language and it is the standard markup language for the web. \n",
    "\n",
    "#### GET Requests Using urllib\n",
    "To get data from a web page, like the wikipedia home page, you specify the URL and package the GET request using the Request() function from the urllib package and then send the request and capture the response using urlopen. The urlopen function call will return an HTTP response object which has an associated read() method. You then apply the read() method to the response which returns the HTML as a string. As a last step, remember to be polite and close the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "url= 'https://www.wikipedia.org'\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests Package\n",
    "The requests package is one of the most popular and widely used Python packages for automating sending http GET requests. Using the requests package can shorten the steps in requesting and retrieving html.Rather than 3 steps to package the request, send the request and receive the response, you can use a single requests.get() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.wikipedia.org'\n",
    "r = requests.get(url)\n",
    "text = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "#### Performing HTTP requests in Python using urllib\n",
    "Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, \"https://campus.datacamp.com/courses/1606/4135?ex=2\".\n",
    "\n",
    "In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.\n",
    "\n",
    "__Instructions:__\n",
    "* Import the functions urlopen and Request from the subpackage urllib.request.\n",
    "* Package the request to the url \"https://campus.datacamp.com/courses/1606/4135?ex=2\" using the function Request() and assign it to request.\n",
    "* Send the request and catch the response in the variable response with the function urlopen().\n",
    "* Run the rest of the code to see the datatype of response and to close the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing HTTP request results in Python using urllib\n",
    "You have just packaged and sent a GET request to \"https://campus.datacamp.com/courses/1606/4135?ex=2\" and then caught the response. You saw that such a response is a http.client.HTTPResponse object. The question remains: what can you do with this response?\n",
    "\n",
    "Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a http.client.HTTPResponse object has an associated read() method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.\n",
    "\n",
    "__Instructions:__\n",
    "* Send the request and catch the response in the variable response with the function urlopen(), as in the previous exercise.\n",
    "* Extract the response using the read() method and store the result in the variable html.\n",
    "* Print the string html.\n",
    "* Hit submit to perform all of the above and to close the response: be tidy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html lang=\"en\"><head><link rel=\"icon\" type=\"image/png\" href=\"/favicon.ico\"><link href=\"/static/css/main.540decb0.css\" rel=\"stylesheet\"><title data-react-helmet=\"true\">Importing flat files from the web: your turn! | Python</title><link data-react-helmet=\"true\" rel=\"canonical\" href=\"https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2\"><link data-react-helmet=\"true\" href=\"data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=\" rel=\"apple-touch-icon\"><link data-react-helmet=\"true\" href=\"data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=\" rel=\"apple-touch-icon-precomposed\"><meta data-react-helmet=\"true\" charset=\"utf-8\"><meta data-react-helmet=\"true\" http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"><meta data-react-helmet=\"true\" name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"><meta data-react-helmet=\"true\" name=\"fragment\" content=\"!\"><meta data-react-helmet=\"true\" name=\"keywords\" content=\"R, Python, Data analysis, interactive, learning\"><meta data-react-helmet=\"true\" name=\"description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:card\" content=\"summary\"><meta data-react-helmet=\"true\" name=\"twitter:site\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" name=\"twitter:description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:creator\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:image:src\" content=\"/public/assets/images/var/twitter_share.png\"><meta data-react-helmet=\"true\" name=\"twitter:domain\" content=\"www.datacamp.com\"><meta data-react-helmet=\"true\" property=\"og:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" property=\"og:image\" content=\"/public/assets/images/var/linkedin_share.png\"><meta data-react-helmet=\"true\" name=\"google-signin-clientid\" content=\"892114885437-01a7plbsu1b2vobuhvnckmmanhb58h3a.apps.googleusercontent.com\"><meta data-react-helmet=\"true\" name=\"google-signin-scope\" content=\"email profile\"><meta data-react-helmet=\"true\" name=\"google-signin-cookiepolicy\" content=\"single_host_origin\"><script data-react-helmet=\"true\" async=\"true\" src=\"https://compliance.datacamp.com/base.js\"></script><script data-react-helmet=\"true\">\\n      var dataLayerContent = {\\n        gtm_version: 2,\\n      };\\n      if (typeof window[\\'dataLayer\\'] === \\'undefined\\') {\\n        window[\\'dataLayer\\'] = [dataLayerContent];\\n      } else {\\n        window[\\'dataLayer\\'].push(dataLayerContent);\\n      }\\n    </script></head><body><script>window.PRELOADED_STATE = \"[&quot;~#iM&quot;,[&quot;contentAuthorization&quot;,[&quot;^0&quot;,[]],&quot;preFetchedData&quot;,[&quot;^0&quot;,[&quot;course&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,1606,&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;description&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;author_field&quot;,null,&quot;author_bio&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;nb_of_subscriptions&quot;,98441,&quot;slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;last_updated_on&quot;,&quot;27/08/2020&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;should_cache&quot;,true,&quot;type&quot;,&quot;datacamp&quot;,&quot;difficulty_level&quot;,1,&quot;state&quot;,&quot;live&quot;,&quot;university&quot;,null,&quot;sharing_links&quot;,[&quot;^ &quot;,&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;],&quot;marketing_video&quot;,&quot;importing-data-in-python-part-2-marketing-video&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;paid&quot;,true,&quot;time_needed&quot;,null,&quot;xp&quot;,2400,&quot;topic_id&quot;,8,&quot;technology_id&quot;,2,&quot;reduced_outline&quot;,null,&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;instructors&quot;,[[&quot;^ &quot;,&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist at DataCamp&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out DataFramed, the DataCamp podcast, which he hosts and produces: https://www.datacamp.com/community/podcast&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/301/837/square/hugoaboutpic.jpg?1493154678&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]],&quot;collaborators&quot;,[[&quot;^ &quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/users/avatars/000/382/294/square/francis-photo.jpg?1471980001&quot;,&quot;^R&quot;,&quot;Francisco Castro&quot;]],&quot;datasets&quot;,[[&quot;^ &quot;,&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;],[&quot;^ &quot;,&quot;^V&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;^W&quot;,&quot;Tweets&quot;],[&quot;^ &quot;,&quot;^V&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;^W&quot;,&quot;Red wine quality&quot;]],&quot;tracks&quot;,[[&quot;^ &quot;,&quot;path&quot;,&quot;/tracks/data-science-for-everyone&quot;,&quot;title_with_subtitle&quot;,&quot;Data Science for Everyone&quot;],[&quot;^ &quot;,&quot;^Y&quot;,&quot;/tracks/data-analyst-with-python&quot;,&quot;^Z&quot;,&quot;Data Analyst  with Python&quot;],[&quot;^ &quot;,&quot;^Y&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;^Z&quot;,&quot;Data Scientist  with Python&quot;],[&quot;^ &quot;,&quot;^Y&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;^Z&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]],&quot;prerequisites&quot;,[[&quot;^ &quot;,&quot;^Y&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;^1&quot;,&quot;Introduction to Importing Data in Python&quot;]],&quot;time_needed_in_hours&quot;,2,&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;seo_description&quot;,&quot;Further improve your Python data importing skills and learn to work with more web and API data.&quot;,&quot;archived_at&quot;,null,&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;chapters&quot;,[[&quot;^ &quot;,&quot;id&quot;,4135,&quot;title_meta&quot;,null,&quot;^1&quot;,&quot;Importing data from the Internet&quot;,&quot;^2&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;number&quot;,1,&quot;^8&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;nb_exercises&quot;,12,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;25/08/2020&quot;,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;free_preview&quot;,true,&quot;xp&quot;,1050,&quot;number_of_videos&quot;,3,&quot;exercises&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Importing flat files  from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;HTTP requests  to import files  from the web&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Scraping the web  in Python&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]],[&quot;^ &quot;,&quot;id&quot;,4136,&quot;^17&quot;,null,&quot;^1&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;^2&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;^18&quot;,2,&quot;^8&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;^19&quot;,9,&quot;^1:&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1;&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;25/08/2020&quot;,&quot;^1&lt;&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;^1=&quot;,null,&quot;xp&quot;,650,&quot;^1&gt;&quot;,2,&quot;^1?&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Introduction to  APIs and JSONs&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Loading and exploring a JSON&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;APIs and  interacting with  the world wide web&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;API requests&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;]]],[&quot;^ &quot;,&quot;id&quot;,4140,&quot;^17&quot;,null,&quot;^1&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;^2&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;^18&quot;,3,&quot;^8&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;^19&quot;,8,&quot;^1:&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1;&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;25/08/2020&quot;,&quot;^1&lt;&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;^1=&quot;,null,&quot;xp&quot;,700,&quot;^1&gt;&quot;,2,&quot;^1?&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;The Twitter API and Authentication&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;API Authentication&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Streaming tweets&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Load and explore your Twitter data&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Twitter data to DataFrame&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Plotting your Twitter data&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Final Thoughts&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=8&quot;]]]]]]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,4135,&quot;^17&quot;,null,&quot;^1&quot;,&quot;Importing data from the Internet&quot;,&quot;^2&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;^18&quot;,1,&quot;^8&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;^19&quot;,12,&quot;^1:&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1;&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;25/08/2020&quot;,&quot;^1&lt;&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;^1=&quot;,true,&quot;xp&quot;,1050,&quot;^1&gt;&quot;,3,&quot;^1?&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Importing flat files  from the web&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;HTTP requests  to import files  from the web&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Scraping the web  in Python&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[[&quot;^ &quot;,&quot;id&quot;,990668,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;assignment&quot;,null,&quot;^1&quot;,&quot;Importing flat files  from the web&quot;,&quot;sample_code&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;^18&quot;,1,&quot;sct&quot;,&quot;&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;attachments&quot;,null,&quot;xp&quot;,50,&quot;possible_answers&quot;,[],&quot;feedbacks&quot;,[],&quot;question&quot;,&quot;&quot;,&quot;video_link&quot;,null,&quot;video_hls&quot;,null,&quot;aspect_ratio&quot;,56.25,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;language&quot;,&quot;python&quot;,&quot;course_id&quot;,1606,&quot;chapter_id&quot;,4135,&quot;^L&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;randomNumber&quot;,0.4008194326817296,&quot;externalId&quot;,990668],[&quot;^ &quot;,&quot;id&quot;,42707,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;http://archive.ics.uci.edu/ml/index.html\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1B&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,2,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.6865063529240394,&quot;^1T&quot;,42707],[&quot;^ &quot;,&quot;id&quot;,42708,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;^1&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1B&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,3,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;pandas.DataFrame.hist\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.08826170410821965,&quot;^1T&quot;,42708],[&quot;^ &quot;,&quot;id&quot;,42709,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1B&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,4,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.5526698374075214,&quot;^1T&quot;,42709],[&quot;^ &quot;,&quot;id&quot;,990669,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1A&quot;,null,&quot;^1&quot;,&quot;HTTP requests  to import files  from the web&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,5,&quot;sct&quot;,&quot;&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1K&quot;,null,&quot;^1L&quot;,null,&quot;^1M&quot;,56.25,&quot;^1N&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,1606,&quot;^1Q&quot;,4135,&quot;^L&quot;,null,&quot;^1R&quot;,&quot;v0&quot;,&quot;^1S&quot;,0.4978977176777801,&quot;^1T&quot;,990669],[&quot;^ &quot;,&quot;id&quot;,42711,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1B&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,6,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.12463794105451353,&quot;^1T&quot;,42711],[&quot;^ &quot;,&quot;id&quot;,42712,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1B&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,7,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.8855734778679341,&quot;^1T&quot;,42712],[&quot;^ &quot;,&quot;id&quot;,42713,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1B&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,8,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.8647431238629242,&quot;^1T&quot;,42713],[&quot;^ &quot;,&quot;id&quot;,990670,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1A&quot;,null,&quot;^1&quot;,&quot;Scraping the web  in Python&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,9,&quot;sct&quot;,&quot;&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1K&quot;,null,&quot;^1L&quot;,null,&quot;^1M&quot;,56.25,&quot;^1N&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;key&quot;,&quot;da43858012&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,1606,&quot;^1Q&quot;,4135,&quot;^L&quot;,null,&quot;^1R&quot;,&quot;v0&quot;,&quot;^1S&quot;,0.1190953342412473,&quot;^1T&quot;,990670],[&quot;^ &quot;,&quot;id&quot;,42715,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1B&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,10,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.7329776883132613,&quot;^1T&quot;,42715],[&quot;^ &quot;,&quot;id&quot;,42716,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1B&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,11,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.6577171021839714,&quot;^1T&quot;,42716],[&quot;^ &quot;,&quot;id&quot;,42717,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1B&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,12,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1S&quot;,0.9396133364432027,&quot;^1T&quot;,42717]]]],&quot;activeImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,&quot;course-1606-master:506759a234ec905a9377923e00ae7511-20200825200848887&quot;]],&quot;sharedImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;NOT_FETCHED&quot;,&quot;data&quot;,null]]]],&quot;systemStatus&quot;,[&quot;^0&quot;,[&quot;indicator&quot;,&quot;none&quot;,&quot;description&quot;,&quot;No status has been fetched from the Status Page.&quot;]],&quot;backendSession&quot;,[&quot;^0&quot;,[&quot;status&quot;,[&quot;^0&quot;,[&quot;code&quot;,&quot;none&quot;,&quot;text&quot;,&quot;&quot;]],&quot;isInitSession&quot;,false,&quot;message&quot;,null]],&quot;settings&quot;,[&quot;^0&quot;,[&quot;uiTheme&quot;,&quot;LIGHT&quot;]],&quot;challengeMode&quot;,[&quot;^0&quot;,[&quot;sampleCodesByExerciseId&quot;,[&quot;^0&quot;,[544,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Select poker results for Monday, Tuesday and Wednesday\\\\\\\\n\\\\\\\\n  \\\\\\\\n# Calculate the average of the elements in poker_start&quot;,545,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Which days did you make money on poker?\\\\\\\\n \\\\\\\\n \\\\\\\\n# Print out selection_vector&quot;,577,&quot;# planets_df is pre-loaded in your workspace\\\\\\\\n\\\\\\\\n# Use order() to create positions\\\\\\\\n\\\\\\\\n\\\\\\\\n# Use positions to sort planets_df&quot;,546,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Which days did you make money on poker?\\\\\\\\nselection_vector &lt;- poker_vector &gt; 0\\\\\\\\n\\\\\\\\n# Select from poker_vector these days&quot;,547,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Which days did you make money on roulette?\\\\\\\\n\\\\\\\\n\\\\\\\\n# Select from roulette_vector these days&quot;,580,&quot;# Vector with numerics from 1 up to 10\\\\\\\\nmy_vector &lt;- 1:10 \\\\\\\\n\\\\\\\\n# Matrix with numerics from 1 up to 9\\\\\\\\nmy_matrix &lt;- matrix(1:9, ncol = 3)\\\\\\\\n\\\\\\\\n# First 10 elements of the built-in data frame mtcars\\\\\\\\nmy_df &lt;- mtcars[1:10,]\\\\\\\\n\\\\\\\\n# Construct list with these different elements:&quot;,549,&quot;# Box office Star Wars (in millions!)\\\\\\\\nnew_hope &lt;- c(460.998, 314.4)\\\\\\\\nempire_strikes &lt;- c(290.475, 247.900)\\\\\\\\nreturn_jedi &lt;- c(309.306, 165.8)\\\\\\\\n\\\\\\\\n# Create box_office\\\\\\\\n\\\\\\\\n\\\\\\\\n# Construct star_wars_matrix&quot;,582,&quot;# The variables mov, act and rev are available\\\\\\\\n\\\\\\\\n# Finish the code to build shining_list&quot;,551,&quot;# Construct star_wars_matrix\\\\\\\\nbox_office &lt;- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\\\\\\\\nstar_wars_matrix &lt;- matrix(box_office, nrow = 3, byrow = TRUE,\\\\\\\\n                           dimnames = list(c(\\\\\\\\&quot;A New Hope\\\\\\\\&quot;, \\\\\\\\&quot;The Empire Strikes Back\\\\\\\\&quot;, \\\\\\\\&quot;Return of the Jedi\\\\\\\\&quot;), \\\\\\\\n                                           c(\\\\\\\\&quot;US\\\\\\\\&quot;, \\\\\\\\&quot;non-US\\\\\\\\&quot;)))\\\\\\\\n\\\\\\\\n# Calculate worldwide box office figures&quot;,552,&quot;# Construct star_wars_matrix\\\\\\\\nbox_office &lt;- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\\\\\\\\nstar_wars_matrix &lt;- matrix(box_office, nrow = 3, byrow = TRUE,\\\\\\\\n                           dimnames = list(c(\\\\\\\\&quot;A New Hope\\\\\\\\&quot;, \\\\\\\\&quot;The Empire Strikes Back\\\\\\\\&quot;, \\\\\\\\&quot;Return of the Jedi\\\\\\\\&quot;), \\\\\\\\n                                           c(\\\\\\\\&quot;US\\\\\\\\&quot;, \\\\\\\\&quot;non-US\\\\\\\\&quot;)))\\\\\\\\n\\\\\\\\n# The worldwide box office figures\\\\\\\\nworldwide_vector &lt;- rowSums(star_wars_matrix)\\\\\\\\n\\\\\\\\n# Bind the new variable worldwide_vector as a column to star_wars_matrix&quot;,553,&quot;# star_wars_matrix and star_wars_matrix2 are available in your workspace\\\\\\\\n\\\\\\\\n# Combine both Star Wars trilogies in one matrix&quot;,554,&quot;# all_wars_matrix is available in your workspace\\\\\\\\n\\\\\\\\n# Select the non-US revenue for all movies\\\\\\\\n\\\\\\\\n  \\\\\\\\n# Average non-US revenue\\\\\\\\n\\\\\\\\n  \\\\\\\\n# Select the non-US revenue for first two movies\\\\\\\\n\\\\\\\\n  \\\\\\\\n# Average non-US revenue for first two movies&quot;,555,&quot;# all_wars_matrix is available in your workspace\\\\\\\\n\\\\\\\\n# Estimate the visitors\\\\\\\\n\\\\\\\\n  \\\\\\\\n# Print the estimate to the console&quot;,556,&quot;# all_wars_matrix and ticket_prices_matrix are available in your workspace\\\\\\\\n\\\\\\\\n# Estimated number of visitors\\\\\\\\n\\\\\\\\n\\\\\\\\n# US visitors\\\\\\\\n\\\\\\\\n\\\\\\\\n# Average number of US visitors&quot;,558,&quot;# Sex vector\\\\\\\\nsex_vector &lt;- c(\\\\\\\\&quot;Male\\\\\\\\&quot;, \\\\\\\\&quot;Female\\\\\\\\&quot;, \\\\\\\\&quot;Female\\\\\\\\&quot;, \\\\\\\\&quot;Male\\\\\\\\&quot;, \\\\\\\\&quot;Male\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Convert sex_vector to a factor\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print out factor_sex_vector&quot;,560,&quot;# Code to build factor_survey_vector\\\\\\\\nsurvey_vector &lt;- c(\\\\\\\\&quot;M\\\\\\\\&quot;, \\\\\\\\&quot;F\\\\\\\\&quot;, \\\\\\\\&quot;F\\\\\\\\&quot;, \\\\\\\\&quot;M\\\\\\\\&quot;, \\\\\\\\&quot;M\\\\\\\\&quot;)\\\\\\\\nfactor_survey_vector &lt;- factor(survey_vector)\\\\\\\\n\\\\\\\\n# Specify the levels of factor_survey_vector&quot;,563,&quot;# Create speed_vector&quot;,4083,&quot;# all_wars_matrix is available in your workspace\\\\\\\\n\\\\\\\\n# Total revenue for US and non-US\\\\\\\\n\\\\\\\\n  \\\\\\\\n# Print out total_revenue_vector&quot;,532,&quot;# Define the variable vegas&quot;,564,&quot;# Create speed_vector\\\\\\\\nspeed_vector &lt;- c(\\\\\\\\&quot;medium\\\\\\\\&quot;, \\\\\\\\&quot;slow\\\\\\\\&quot;, \\\\\\\\&quot;slow\\\\\\\\&quot;, \\\\\\\\&quot;medium\\\\\\\\&quot;, \\\\\\\\&quot;fast\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Convert speed_vector to ordered factor vector\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print factor_speed_vector&quot;,533,&quot;numeric_vector &lt;- c(1, 10, 49)\\\\\\\\ncharacter_vector &lt;- c(\\\\\\\\&quot;a\\\\\\\\&quot;, \\\\\\\\&quot;b\\\\\\\\&quot;, \\\\\\\\&quot;c\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Complete the code for boolean_vector&quot;,565,&quot;# Create factor_speed_vector\\\\\\\\nspeed_vector &lt;- c(\\\\\\\\&quot;medium\\\\\\\\&quot;, \\\\\\\\&quot;slow\\\\\\\\&quot;, \\\\\\\\&quot;slow\\\\\\\\&quot;, \\\\\\\\&quot;medium\\\\\\\\&quot;, \\\\\\\\&quot;fast\\\\\\\\&quot;)\\\\\\\\nfactor_speed_vector &lt;- factor(speed_vector, ordered = TRUE, levels = c(\\\\\\\\&quot;slow\\\\\\\\&quot;, \\\\\\\\&quot;medium\\\\\\\\&quot;, \\\\\\\\&quot;fast\\\\\\\\&quot;))\\\\\\\\n\\\\\\\\n# Factor value for second data analyst\\\\\\\\n\\\\\\\\n\\\\\\\\n# Factor value for fifth data analyst\\\\\\\\n\\\\\\\\n\\\\\\\\n# Is data analyst 2 faster than data analyst 5?&quot;,534,&quot;# Poker winnings from Monday to Friday\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\n\\\\\\\\n# Roulette winnings from Monday to Friday&quot;,536,&quot;# Poker winnings from Monday to Friday\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\n\\\\\\\\n# Roulette winnings from Monday to Friday\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\n\\\\\\\\n# The variable days_vector\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\n \\\\\\\\n# Assign the names of the day to roulette_vector and poker_vector&quot;,537,&quot;A_vector &lt;- c(1, 2, 3)\\\\\\\\nB_vector &lt;- c(4, 5, 6)\\\\\\\\n\\\\\\\\n# Take the sum of A_vector and B_vector\\\\\\\\n  \\\\\\\\n# Print out total_vector&quot;,569,&quot;# Definition of vectors\\\\\\\\nname &lt;- c(\\\\\\\\&quot;Mercury\\\\\\\\&quot;, \\\\\\\\&quot;Venus\\\\\\\\&quot;, \\\\\\\\&quot;Earth\\\\\\\\&quot;, \\\\\\\\&quot;Mars\\\\\\\\&quot;, \\\\\\\\&quot;Jupiter\\\\\\\\&quot;, \\\\\\\\&quot;Saturn\\\\\\\\&quot;, \\\\\\\\&quot;Uranus\\\\\\\\&quot;, \\\\\\\\&quot;Neptune\\\\\\\\&quot;)\\\\\\\\ntype &lt;- c(\\\\\\\\&quot;Terrestrial planet\\\\\\\\&quot;, \\\\\\\\&quot;Terrestrial planet\\\\\\\\&quot;, \\\\\\\\&quot;Terrestrial planet\\\\\\\\&quot;, \\\\\\\\n          \\\\\\\\&quot;Terrestrial planet\\\\\\\\&quot;, \\\\\\\\&quot;Gas giant\\\\\\\\&quot;, \\\\\\\\&quot;Gas giant\\\\\\\\&quot;, \\\\\\\\&quot;Gas giant\\\\\\\\&quot;, \\\\\\\\&quot;Gas giant\\\\\\\\&quot;)\\\\\\\\ndiameter &lt;- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883)\\\\\\\\nrotation &lt;- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67)\\\\\\\\nrings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\\\\\\\\n\\\\\\\\n# Create a data frame from the vectors&quot;,538,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Assign to total_daily how much you won/lost on each day&quot;,539,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Total winnings with poker\\\\\\\\ntotal_poker &lt;- sum(poker_vector)\\\\\\\\n\\\\\\\\n# Total winnings with roulette\\\\\\\\n\\\\\\\\n\\\\\\\\n# Total winnings overall\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print out total_week&quot;,540,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Calculate total gains for poker and roulette\\\\\\\\n\\\\\\\\n\\\\\\\\n# Check if you realized higher total gains in poker than in roulette&quot;,541,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Define a new variable based on a selection&quot;,573,&quot;# planets_df is pre-loaded in your workspace\\\\\\\\n\\\\\\\\n# Select the rings variable from planets_df\\\\\\\\n\\\\\\\\n  \\\\\\\\n# Print out rings_vector&quot;,542,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Define a new variable based on a selection&quot;,543,&quot;# Poker and roulette winnings from Monday to Friday:\\\\\\\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\\\\\\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\\\\\\\ndays_vector &lt;- c(\\\\\\\\&quot;Monday\\\\\\\\&quot;, \\\\\\\\&quot;Tuesday\\\\\\\\&quot;, \\\\\\\\&quot;Wednesday\\\\\\\\&quot;, \\\\\\\\&quot;Thursday\\\\\\\\&quot;, \\\\\\\\&quot;Friday\\\\\\\\&quot;)\\\\\\\\nnames(poker_vector) &lt;- days_vector\\\\\\\\nnames(roulette_vector) &lt;- days_vector\\\\\\\\n\\\\\\\\n# Define a new variable based on a selection&quot;]],&quot;isActive&quot;,false]],&quot;autocomplete&quot;,[&quot;^0&quot;,[]],&quot;mobilePopup&quot;,[&quot;^0&quot;,[]],&quot;user&quot;,[&quot;^0&quot;,[&quot;status&quot;,null,&quot;settings&quot;,[&quot;^0&quot;,[]]]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;25/08/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;~#iL&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files  from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests  to import files  from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web  in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;boot&quot;,[&quot;^0&quot;,[&quot;bootState&quot;,&quot;PRE_BOOTED&quot;,&quot;error&quot;,null]],&quot;location&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^0&quot;,[&quot;ex&quot;,&quot;2&quot;]]]],&quot;canonical&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;before&quot;,[&quot;^0&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^0&quot;,[&quot;ex&quot;,&quot;2&quot;]]]]]],&quot;authorization&quot;,[&quot;^ &quot;],&quot;course&quot;,[&quot;^0&quot;,[&quot;difficulty_level&quot;,1,&quot;reduced_outline&quot;,null,&quot;marketing_video&quot;,&quot;importing-data-in-python-part-2-marketing-video&quot;,&quot;active_image&quot;,&quot;course-1606-master:506759a234ec905a9377923e00ae7511-20200825200848887&quot;,&quot;author_field&quot;,null,&quot;chapters&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;25/08/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files  from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests  to import files  from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web  in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;last_updated_on&quot;,&quot;25/08/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,9,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;title&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;xp&quot;,650,&quot;id&quot;,4136,&quot;exercises&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Introduction to  APIs and JSONs&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Loading and exploring a JSON&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;APIs and  interacting with  the world wide web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;API requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;]]]],&quot;description&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;last_updated_on&quot;,&quot;25/08/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,8,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;title&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;xp&quot;,700,&quot;id&quot;,4140,&quot;exercises&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;The Twitter API and Authentication&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;API Authentication&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Streaming tweets&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Load and explore your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Twitter data to DataFrame&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Final Thoughts&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=8&quot;]]]],&quot;description&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;time_needed&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;tracks&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/data-science-for-everyone&quot;,&quot;title_with_subtitle&quot;,&quot;Data Science for Everyone&quot;]],[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/data-analyst-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Analyst  with Python&quot;]],[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Scientist  with Python&quot;]],[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]]]],&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;topic_id&quot;,8,&quot;slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;last_updated_on&quot;,&quot;27/08/2020&quot;,&quot;paid&quot;,true,&quot;collaborators&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/382/294/square/francis-photo.jpg?1471980001&quot;,&quot;full_name&quot;,&quot;Francisco Castro&quot;]]]],&quot;time_needed_in_hours&quot;,2,&quot;technology_id&quot;,2,&quot;university&quot;,null,&quot;archived_at&quot;,null,&quot;state&quot;,&quot;live&quot;,&quot;author_bio&quot;,null,&quot;should_cache&quot;,true,&quot;sharing_links&quot;,[&quot;^0&quot;,[&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;]],&quot;instructors&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist at DataCamp&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out DataFramed, the DataCamp podcast, which he hosts and produces: https://www.datacamp.com/community/podcast&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/301/837/square/hugoaboutpic.jpg?1493154678&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]]]],&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;xp&quot;,2400,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;nb_of_subscriptions&quot;,98441,&quot;seo_description&quot;,&quot;Further improve your Python data importing skills and learn to work with more web and API data.&quot;,&quot;type&quot;,&quot;datacamp&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;id&quot;,1606,&quot;datasets&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;]],[&quot;^0&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;name&quot;,&quot;Tweets&quot;]],[&quot;^0&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;name&quot;,&quot;Red wine quality&quot;]]]],&quot;description&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;prerequisites&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;path&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;title&quot;,&quot;Introduction to Importing Data in Python&quot;]]]],&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;current&quot;,1,&quot;all&quot;,[&quot;^1U&quot;,[[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990668,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,1,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.4008194326817296,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Importing flat files  from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990668,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;course_id&quot;,1606]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42707,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,2,&quot;user&quot;,[&quot;^0&quot;,[&quot;isHintShown&quot;,false,&quot;editorTabs&quot;,[&quot;^0&quot;,[&quot;files/script.py&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;script.py&quot;,&quot;isSolution&quot;,false,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isClosable&quot;,false,&quot;code&quot;,null,&quot;extra&quot;,[&quot;^0&quot;,[]]]]]]]],&quot;outputMarkdownTabs&quot;,[&quot;^0&quot;,[]],&quot;markdown&quot;,[&quot;^0&quot;,[&quot;titles&quot;,[&quot;^1U&quot;,[&quot;Knit PDF&quot;,&quot;Knit HTML&quot;]],&quot;activeTitle&quot;,&quot;Knit HTML&quot;]],&quot;currentXp&quot;,100,&quot;graphicalTabs&quot;,[&quot;^0&quot;,[&quot;plot&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;Plots&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^1U&quot;,[]],&quot;currentIndex&quot;,0]],&quot;dimension&quot;,[&quot;^0&quot;,[&quot;isRealSize&quot;,false,&quot;width&quot;,1,&quot;height&quot;,1]]]],&quot;html&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;HTML Viewer&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^1U&quot;,[]],&quot;currentIndex&quot;,0]]]]]],&quot;feedbackMessages&quot;,[&quot;^1U&quot;,[]],&quot;lastSubmittedCode&quot;,null,&quot;ltiStatus&quot;,[&quot;^0&quot;,[]],&quot;lastSubmitActiveEditorTab&quot;,null,&quot;consoleSqlTabs&quot;,[&quot;^0&quot;,[&quot;query_result&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;&quot;,&quot;title&quot;,&quot;query result&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isNotView&quot;,true,&quot;message&quot;,&quot;No query executed yet...&quot;]]]]]],&quot;consoleTabs&quot;,[&quot;^0&quot;,[&quot;console&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;IPython Shell&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true]],&quot;dimension&quot;,[&quot;^0&quot;,[]]]],&quot;slides&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;Slides&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,false]]]]]],&quot;inputMarkdownTabs&quot;,[&quot;^0&quot;,[]],&quot;consoleObjectViewTabs&quot;,[&quot;^0&quot;,[]]]],&quot;randomNumber&quot;,0.6865063529240394,&quot;assignment&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;http://archive.ics.uci.edu/ml/index.html\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42707]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;pandas.DataFrame.hist\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42708,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,3,&quot;randomNumber&quot;,0.08826170410821965,&quot;assignment&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42708]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42709,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,4,&quot;randomNumber&quot;,0.5526698374075214,&quot;assignment&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42709]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990669,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,5,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.4978977176777801,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;HTTP requests  to import files  from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990669,&quot;projector_key&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;course_id&quot;,1606]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42711,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,6,&quot;randomNumber&quot;,0.12463794105451353,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42711]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42712,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,7,&quot;randomNumber&quot;,0.8855734778679341,&quot;assignment&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42712]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42713,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,8,&quot;randomNumber&quot;,0.8647431238629242,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42713]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990670,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,9,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.1190953342412473,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Scraping the web  in Python&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990670,&quot;projector_key&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;da43858012&quot;,&quot;course_id&quot;,1606]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42715,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,10,&quot;randomNumber&quot;,0.7329776883132613,&quot;assignment&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42715]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42716,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,11,&quot;randomNumber&quot;,0.6577171021839714,&quot;assignment&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42716]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42717,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1U&quot;,[]],&quot;number&quot;,12,&quot;randomNumber&quot;,0.9396133364432027,&quot;assignment&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1U&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42717]]]],&quot;canRateChapter&quot;,false,&quot;isChapterCompleted&quot;,false]],&quot;onboardingMilestones&quot;,[&quot;^0&quot;,[&quot;isActive&quot;,true,&quot;step&quot;,0]]]]\";</script><div id=\"root\"><div class=\"theme progress-indicator--visible theme--light\"><header data-cy=\"header-container\" class=\"dc-header-campus\"><a href=\"https://www.datacamp.com\" data-cy=\"header-logo\" class=\"dc-header-campus__home\"><img alt=\"datacamp-logo\" src=\"/static/media/logo-full-color.018b48cc.svg\" style=\"max-height:29px\"></a><div class=\"dc-nav-course__container\"><nav class=\"dc-nav-course\"><a data-cy=\"header-previous\" class=\"dc-nav-course__backward\" data-tip=\"true\" data-for=\"nav-tp-prev\" href=\"/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1\"><svg width=\"12\" height=\"12\" aria-label=\"arrow_2_left icon\" class=\"dc-icon-arrow_2_left dc-nav-course__icon\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#arrow_2_left\"/></svg></a><a href=\"javascript:void(0)\" data-cy=\"header-outline\" class=\"dc-nav-course__outline\" data-tip=\"true\" data-for=\"nav-tp-outline\"><svg width=\"12\" height=\"12\" aria-label=\"bars icon\" class=\"dc-icon-bars dc-nav-course__icon\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#bars\"/></svg>Course Outline</a><a data-cy=\"header-next\" class=\"dc-nav-course__forward\" data-tip=\"true\" data-for=\"nav-tp-next\" href=\"/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3\"><svg width=\"12\" height=\"12\" aria-label=\"arrow_2_right icon\" class=\"dc-icon-arrow_2_right dc-nav-course__icon\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#arrow_2_right\"/></svg></a></nav></div><nav class=\"dc-u-fx dc-u-fx-aic dc-u-fx-jcfe dc-u-w-96\"><div data-cy=\"header-session\" class=\"app-status dc-u-fx dc-u-mr-8\"><div class=\"hcSlide-wrapper\"></div><svg width=\"18\" height=\"18\" aria-label=\"circle icon\" class=\"dc-icon-circle dc-u-color-green\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#circle\"/></svg></div><a href=\"javascript:void(0)\" data-cy=\"header-video\" class=\"ds-icon-action dc-u-fx\"><svg tooltip-id=\"tp-video\" tooltip-place=\"left\" width=\"18\" height=\"18\" aria-label=\"video icon\" class=\"dc-icon-video dc-u-fx\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#video\"/></svg></a><a href=\"javascript:void(0)\" data-cy=\"header-slides\" class=\"ds-icon-action dc-u-fx\"><svg tooltip-id=\"tp-slides\" tooltip-place=\"left\" width=\"18\" height=\"18\" aria-label=\"pdf icon\" class=\"dc-icon-pdf dc-u-fx\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#pdf\"/></svg></a><a href=\"javascript:void(0)\" data-cy=\"header-issue\" class=\"ds-icon-action dc-u-fx\" data-test-id=\"header-report-issue-button\"><svg tooltip-place=\"left\" tooltip-id=\"tp-issue\" width=\"18\" height=\"18\" aria-label=\"attention icon\" class=\"dc-icon-attention dc-u-fx\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#attention\"/></svg></a></nav></header><div class=\"exercise-area\"><div data-cy=\"server-side-loader-placeholder\"><aside class=\"exercise--sidebar\" style=\"width:40%\"><div class=\"exercise--sidebar-content\"><div class=\"listview__outer\"><div class=\"listview__inner\"><div class=\"listview__section\"><div><div role=\"button\" class=\"listview__header\"><div class=\"exercise--sidebar-header\"><h5 class=\"dc-panel__title\"><svg width=\"12\" height=\"12\" aria-label=\"exercise icon\" class=\"dc-icon-exercise dc-u-color-grey-dark dc-u-mr-8\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#exercise\"/></svg>Exercise</h5></div></div></div><div class=\"listview__content\"><div class=\"exercise--assignment exercise--typography\"><h1 class=\"exercise--title\">Importing flat files from the web: your turn!</h1><div class><p>You are about to import your first file from the web! The flat file you will import will be <code>&apos;winequality-red.csv&apos;</code> from the University of California, Irvine&apos;s <a href=\"http://archive.ics.uci.edu/ml/index.html\">Machine Learning repository</a>. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.</p>\\n<p>The URL of the file is</p>\\n<pre><code>&apos;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&apos;\\n</code></pre>\\n<p>After you import it, you&apos;ll check your working directory to confirm that it is there and then you&apos;ll load it into a <code>pandas</code> DataFrame.</p></div></div></div></div><div class=\"listview__section\" style=\"min-height:calc(100% - 33px)\"><div><div role=\"button\" class=\"listview__header\"><div class=\"exercise--sidebar-header\"><h5 class=\"dc-panel__title\"><svg width=\"12\" height=\"12\" aria-label=\"checkmark_circle icon\" class=\"dc-icon-checkmark_circle dc-u-color-grey-dark dc-u-mr-8\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#checkmark_circle\"/></svg>Instructions</h5><span class=\"dc-tag tag--xp\">100<!-- --> XP</span></div></div></div><div class=\"listview__content\"><div><div class><div class=\"exercise--instructions exercise--typography\"><div class=\"exercise--instructions__content\"><ul>\\n<li>Import the function <code>urlretrieve</code> from the subpackage <code>urllib.request</code>.</li>\\n<li>Assign the URL of the file to the variable <code>url</code>.</li>\\n<li>Use the function <code>urlretrieve()</code> to save the file locally as <code>&apos;winequality-red.csv&apos;</code>.</li>\\n<li>Execute the remaining code to load <code>&apos;winequality-red.csv&apos;</code> in a pandas DataFrame and to print its head to the shell.</li>\\n</ul></div><div class=\"campus-dc-sct-feedback\" tabindex=\"-1\"><div></div><ul class=\"campus-dc-sct-feedback__tab-list\"><a href=\"javascript:void(0)\" class=\"exercise--show-hint\" tooltip-id=\"tp-hint\" tooltip-wrapperstyle=\"[object Object]\" data-cy=\"exercise-show-hint\"><svg width=\"18\" height=\"18\" aria-label=\"lightbulb icon\" class=\"dc-icon-lightbulb dc-u-mr-4\" role=\"Img\" fill=\"currentColor\"><use xlink:href=\"/static/media/symbols.e369b265.svg#lightbulb\"/></svg><span>Take Hint<!-- --> (-<!-- -->30<!-- --> XP)</span></a></ul></div></div></div></div></div></div></div></div></div></aside><section class=\"exercise--content\" style=\"width:60%\"><div class=\"exercise-waiting\"><div class=\"global-spinner\"><object type=\"image/svg+xml\" data=\"/static/media/spinner.dd0612cb.svg\" aria-label=\"Loading\"></object></div><noscript></noscript></div></section></div><div class=\"Toastify\"></div></div><div class=\"exercise-footer\"><ul data-cy=\"progress-container\" class=\"dc-progress-indicator\"><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li></ul></div><style data-emotion-css=\"rfchpx-contentStyleAfterOpen 1qnoxis-contentStyle 1mud6du-contentStyleBeforeClose 1frn5e-overlayStyleAfterOpen 1hw2u6x-overlayStyle egdnht-overlayStyleBeforeClose i9eya4-bodyOpenStyle\">.css-rfchpx-contentStyleAfterOpen{opacity:1 !important;-webkit-transform:scale(1) !important;-ms-transform:scale(1) !important;transform:scale(1) !important;}.css-1qnoxis-contentStyle{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;min-width:0;opacity:0;outline:none;-webkit-transform:scale(0.5);-ms-transform:scale(0.5);transform:scale(0.5);-webkit-transition:0.4s cubic-bezier(0.19,1,0.22,1);transition:0.4s cubic-bezier(0.19,1,0.22,1);box-sizing:border-box;max-height:100%;padding:8px;width:496px;}.css-1mud6du-contentStyleBeforeClose{opacity:0 !important;}.css-1frn5e-overlayStyleAfterOpen{opacity:1 !important;}.css-1hw2u6x-overlayStyle{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(38,62,99,0.8);bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;-webkit-transition:opacity 0.6s cubic-bezier(0.19,1,0.22,1);transition:opacity 0.6s cubic-bezier(0.19,1,0.22,1);z-index:100;}.css-egdnht-overlayStyleBeforeClose{opacity:0 !important;}.css-i9eya4-bodyOpenStyle{overflow:hidden;}</style></div></div><script type=\"text/x-mathjax-config\">MathJax && MathJax.Hub && MathJax.Hub.Config && MathJax.Hub.Config({\\n        messageStyle: \"none\"\\n      });</script><script type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"></script><script src=\"/static/js/main.43989acf.js\"></script></body></html>'\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing HTTP requests in Python using requests\n",
    "Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their \"http://www.datacamp.com/teach/documentation\" page.\n",
    "\n",
    "Note that unlike in the previous exercises using urllib, you don't have to close the connection when using requests!\n",
    "\n",
    "__Instructions:__\n",
    "* Import the package requests.\n",
    "* Assign the URL of interest to the variable url.\n",
    "* Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.\n",
    "* Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable text.\n",
    "* Hit submit to print the HTML of the webpage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"en\" data-direction=\"ltr\">\n",
      "  <head>\n",
      "    <link href=\"https://fonts.intercomcdn.com\" rel=\"preconnect\" crossorigin>\n",
      "      <script src=\"https://www.googletagmanager.com/gtag/js?id=UA-39297847-9\" async=\"async\" nonce=\"jc63n4PV4I/DWFe5pwm4DvUthDBy5Ltpoea8uJqSLoM=\"></script>\n",
      "      <script nonce=\"jc63n4PV4I/DWFe5pwm4DvUthDBy5Ltpoea8uJqSLoM=\">\n",
      "        window.dataLayer = window.dataLayer || [];\n",
      "        function gtag(){dataLayer.push(arguments);}\n",
      "        gtag('js', new Date());\n",
      "        gtag('config', 'UA-39297847-9');\n",
      "</script>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "    <title>DataCamp Help Center</title>\n",
      "    <meta name=\"description\" content=\"\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "\n",
      "      <link rel=\"alternate\" href=\"http://instructor-support.datacamp.com/en/\" hreflang=\"en\">\n",
      "\n",
      "      <meta name=\"intercom:trackingEvent\" content=\"{&quot;name&quot;:&quot;Viewed Help Center&quot;,&quot;metadata&quot;:{&quot;action&quot;:&quot;viewed&quot;,&quot;object&quot;:&quot;educate_home&quot;,&quot;place&quot;:&quot;help_center&quot;,&quot;owner&quot;:&quot;educate&quot;,&quot;default_locale&quot;:&quot;en&quot;,&quot;current_locale&quot;:&quot;en&quot;,&quot;is_default_locale&quot;:true}}\" />\n",
      "\n",
      "    <link rel=\"stylesheet\" media=\"all\" href=\"https://static.intercomassets.com/alexandria/assets/application-a0d181a060dc622679c644d700744b550cdb0cc6c5ec852e1bde4058a4d02adc.css\" />\n",
      "    <link rel=\"canonical\" href=\"http://instructor-support.datacamp.com/en/\"/>\n",
      "\n",
      "        <link href=\"https://static.intercomassets.com/assets/educate/educate-favicon-64x64-at-2x-52016a3500a250d0b118c0a04ddd13b1a7364a27759483536dd1940bccdefc20.png\" rel=\"shortcut icon\" type=\"image/png\" />\n",
      "      <style>\n",
      "        .header, .avatar__image-extra { background-color: #263e63; }\n",
      "        .article a, .c__primary { color: #263e63; }\n",
      "        .avatar__fallback { background-color: #263e63; }\n",
      "        article a.intercom-h2b-button { background-color: #263e63; border: 0; }\n",
      "      </style>\n",
      "\n",
      "      <meta property=\"og:title\" content=\"DataCamp Help Center\" />\n",
      "  <meta name=\"twitter:title\" content=\"DataCamp Help Center\" />\n",
      "\n",
      "\n",
      "<meta property=\"og:type\" content=\"website\" />\n",
      "<meta property=\"og:image\" content=\"\" />\n",
      "\n",
      "<meta name=\"twitter:image\" content=\"\" />\n",
      "\n",
      "  </head>\n",
      "  <body class=\"\">\n",
      "    <header class=\"header\">\n",
      "  <div class=\"container header__container o__ltr\" dir=\"ltr\">\n",
      "    <div class=\"content\">\n",
      "      <div class=\"mo o__centered o__reversed header__meta_wrapper\">\n",
      "        <div class=\"mo__body header__site_name\">\n",
      "          <div class=\"header__logo\">\n",
      "            <a href=\"/en/\">\n",
      "                <img alt=\"DataCamp Help Center\" src=\"https://downloads.intercomcdn.com/i/o/81221/856b63d438031754b681746b/4ea2737e4266936fb423911d9c587812.png\" />\n",
      "            </a>\n",
      "          </div>\n",
      "        </div>\n",
      "        <div class=\"mo__aside\">\n",
      "          <div class=\"header__links\">\n",
      "              <a target=\"_blank\" rel='noopener' href=\"http://www.datacamp.com/teach\" class=\"header__home__url\"><svg width=\"14\" height=\"14\" viewBox=\"0 0 14 14\" xmlns=\"http://www.w3.org/2000/svg\"><title>Group 65</title><g stroke=\"#FFF\" fill=\"none\" fill-rule=\"evenodd\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M11.5 6.73v6.77H.5v-11h7.615M4.5 9.5l7-7M13.5 5.5v-5h-5\"/></g></svg><span>Go to DataCamp</span></a>\n",
      "            \n",
      "          </div>\n",
      "        </div>\n",
      "      </div>\n",
      "          <h1 class=\"header__headline\">Advice and answers from the DataCamp Team</h1>\n",
      "      <form action=\"/en/\" autocomplete=\"off\" class=\"header__form search\">\n",
      "        <input type=\"text\" autocomplete=\"off\" class=\"search__input js__search-input o__ltr\" placeholder=\"Search for articles...\" tabindex=\"1\" name=\"q\" value=\"\">\n",
      "        <div class=\"search_icons\">\n",
      "          <button type=\"submit\" class=\"search__submit o__ltr\"></button>\n",
      "          <a class=\"search__clear-text__icon\">\n",
      "            <svg class=\"interface-icon\" xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 16 16\">\n",
      "              <path d=\"M8.018 6.643L5.375 4 4 5.375l2.643 2.643L4 10.643 5.375 12l2.643-2.625L10.625 12 12 10.643 9.357 8.018 12 5.375 10.643 4z\" />\n",
      "            </svg>\n",
      "          </a>\n",
      "      </form>\n",
      "      </div>\n",
      "    </div>\n",
      "  </div>\n",
      "</header>\n",
      "\n",
      "    <div class=\"container\">\n",
      "      <div class=\"content educate_content\"><section class=\"section\">\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1256585-getting-started\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"chat-star\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linejoin=\"round\"><path d=\"M20 34.942c-2.083-.12-4.292-.42-6-.942L3 39l4-9c-3.858-3.086-6-7.246-6-12C1 8.61 10.328 1 21.835 1 33.343 1 43 8.61 43 18c0 1.044-.117 2.065-.342 3.057\"></path><path d=\"M36.016 25L40 33h7l-6 5 3 9-8-5.494L28 47l3-9-6-5h7l4.016-8z\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Getting Started</h2>\n",
      "            <p class=\"paper__preview\">Everything you need to know to begin your DataCamp journey!</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2352718/square_128/Rebecca_Robins_-_Headshot-1535969735.jpg?1535969735\" alt=\"Becca Robins avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2637958/square_128/YR_Headshot-1539175806.JPG?1539175806\" alt=\"Yashas Roy avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2734728/square_128/Anneleen_Beckers-xtra-small-1541624054.jpg?1541624054\" alt=\"Anneleen Beckers avatar\" class=\"avatar__image\">\n",
      "\n",
      "      <span class=\"avatar__image avatar__fallback\">+1</span>\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        11 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Becca Robins,</span> <span class='c__darker'> Yashas Roy,</span> <span class='c__darker'> Anneleen Beckers</span> and 1 other\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1256578-courses\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"devices-laptop\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linecap=\"round\"><path d=\"M41 31H7V11h34v20z\"></path><path d=\"M3 35V10a3 3 0 0 1 3-3h36a3 3 0 0 1 3 3v25m-16 0v2H19v-2H1v4a2 2 0 0 0 2 2h42a2 2 0 0 0 2-2v-4H29z\" stroke-linejoin=\"round\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Courses</h2>\n",
      "            <p class=\"paper__preview\">Everything you need to know about creating DataCamp courses.</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <span class=\"avatar__image avatar__fallback\"> J </span>\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2697344/square_128/8ebazuB9_400x400-1559641918.jpg?1559641918\" alt=\"Ramnath Vaidyanathan avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2352718/square_128/Rebecca_Robins_-_Headshot-1535969735.jpg?1535969735\" alt=\"Becca Robins avatar\" class=\"avatar__image\">\n",
      "\n",
      "      <span class=\"avatar__image avatar__fallback\">+6</span>\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        86 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Joyce Chiu,</span> <span class='c__darker'> Ramnath Vaidyanathan,</span> <span class='c__darker'> Becca Robins</span> and 6 others\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1256594-daily-practice\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"tools-dashboard\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M27 31a3 3 0 0 1-6 0 3 3 0 0 1 6 0zm-.88-2.12l9.9-9.9M5 32h4m34 .002L39 32m2.553-8.27l-3.696 1.53M31.27 13.447l-1.53 3.695M24 12v4m-7.27-2.553l1.53 3.695m-7.694.422l2.826 2.83M6.447 23.73l3.695 1.53\"></path><path d=\"M24 8C11.297 8 1 18.3 1 31v9h46v-9C47 18.3 36.703 8 24 8z\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Daily Practice</h2>\n",
      "            <p class=\"paper__preview\">Everything you need to know about creating DataCamp Daily Practice.</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2734728/square_128/Anneleen_Beckers-xtra-small-1541624054.jpg?1541624054\" alt=\"Anneleen Beckers avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        15 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Anneleen Beckers</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1256569-projects\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"book-opened2\"><path d=\"M24 11c0-3.866 10.297-7 23-7v33c-12.703 0-23 3.134-23 7 0-3.866-10.3-7-23-7V4c12.7 0 23 3.134 23 7zm0 0v32m-5-27.52c-3.22-1.232-7.773-2.128-13-2.48m13 8.48c-3.22-1.232-7.773-2.128-13-2.48m13 8.48c-3.22-1.232-7.773-2.128-13-2.48m13 8.48c-3.22-1.23-7.773-2.127-13-2.48m23-15.52c3.223-1.232 7.773-2.128 13-2.48m-13 8.48c3.223-1.232 7.773-2.128 13-2.48m-13 8.48c3.223-1.232 7.773-2.128 13-2.48m-13 8.48c3.223-1.23 7.773-2.127 13-2.48\" stroke-width=\"2\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Projects</h2>\n",
      "            <p class=\"paper__preview\">Everything you need to know about creating DataCamp projects.</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2366194/square_128/richie-in-hairnet-1537451295.JPG?1537451295\" alt=\"Richie Cotton avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2352718/square_128/Rebecca_Robins_-_Headshot-1535969735.jpg?1535969735\" alt=\"Becca Robins avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        20 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Richie Cotton</span> and <span class='c__darker'> Becca Robins</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1256556-course-editor-basics\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"book-bookmark\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linecap=\"round\"><path d=\"M35 31l-6-6-6 6V7h12v24z\"></path><path d=\"M35 9h6v38H11a4 4 0 0 1-4-4V5\" stroke-linejoin=\"round\"></path><path d=\"M39 9V1H11a4 4 0 0 0 0 8h12\" stroke-linejoin=\"round\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Course Editor Basics</h2>\n",
      "            <p class=\"paper__preview\">Everything you need to know to get going with our online course editor.</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2914180/square_128/Sara_Billen_Photo-1555505647.png?1555505647\" alt=\"Sara Billen avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2352718/square_128/Rebecca_Robins_-_Headshot-1535969735.jpg?1535969735\" alt=\"Becca Robins avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2678765/square_128/peterson-1539265042.jpg?1539265042\" alt=\"Amy Peterson avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        5 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Sara Billen,</span> <span class='c__darker'> Becca Robins,</span> and <span class='c__darker'> Amy Peterson</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/2184250-live-courses-and-maintenance\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"book-bookmark\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linecap=\"round\"><path d=\"M35 31l-6-6-6 6V7h12v24z\"></path><path d=\"M35 9h6v38H11a4 4 0 0 1-4-4V5\" stroke-linejoin=\"round\"></path><path d=\"M39 9V1H11a4 4 0 0 0 0 8h12\" stroke-linejoin=\"round\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Live Courses and Maintenance </h2>\n",
      "            <p class=\"paper__preview\">Everything you need to know about maintaining your Live Content on DataCamp</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/3848524/square_128/profile_pic-1583526870.jpg?1583526870\" alt=\"Kelsey McNeillie avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        25 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Kelsey McNeillie</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/2052717-instructor-advisory-board\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"chat-star\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linejoin=\"round\"><path d=\"M20 34.942c-2.083-.12-4.292-.42-6-.942L3 39l4-9c-3.858-3.086-6-7.246-6-12C1 8.61 10.328 1 21.835 1 33.343 1 43 8.61 43 18c0 1.044-.117 2.065-.342 3.057\"></path><path d=\"M36.016 25L40 33h7l-6 5 3 9-8-5.494L28 47l3-9-6-5h7l4.016-8z\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Instructor Advisory Board</h2>\n",
      "            <p class=\"paper__preview\">Learn more about the DataCamp Instructor Advisory Board</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2678519/square_128/pic2-1539176502.JPG?1539176502\" alt=\"Jen Bricker avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        2 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Jen Bricker</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/2399951-live-training-sessions\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"devices-desktop\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linecap=\"round\"><path d=\"M47 36a3 3 0 0 1-3 3H4a3 3 0 0 1-3-3V6a3 3 0 0 1 3-3h40a3 3 0 0 1 3 3v30zm-37 9h28m-7 0H17l2-6h10l2 6z\" stroke-linejoin=\"round\"></path><path d=\"M43 31H5V7h38v24zm-18 4a1 1 0 1 1-2 0 1 1 0 0 1 2 0z\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Live Training Sessions</h2>\n",
      "            <p class=\"paper__preview\">All articles related to the DataCamp Live Training Session program</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/3848524/square_128/profile_pic-1583526870.jpg?1583526870\" alt=\"Kelsey McNeillie avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        10 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Kelsey McNeillie</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1335793-tips-tricks\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"comms-mail\" stroke-width=\"2\" fill=\"none\" fill-rule=\"evenodd\" stroke-linejoin=\"round\"><path d=\"M47 3L1 22l18 7L47 3z\"></path><path d=\"M47 3l-8 37-20-11L47 3zM19 29v16l7-12\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Tips &amp; Tricks</h2>\n",
      "            <p class=\"paper__preview\">Become a DataCamp wizard!</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2352718/square_128/Rebecca_Robins_-_Headshot-1535969735.jpg?1535969735\" alt=\"Becca Robins avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/3421044/square_128/19412139-1572371206.png?1572371206\" alt=\"Maggie Matsui avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        6 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Becca Robins</span> and <span class='c__darker'> Maggie Matsui</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1256565-frequently-asked-questions-faq\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"chat-question\" fill=\"none\" fill-rule=\"evenodd\"><path d=\"M47 21.268c0 10.363-10.297 18.765-23 18.765-2.835 0-5.55-.418-8.058-1.184L2.725 45 7.9 34.668c-4.258-3.406-6.9-8.15-6.9-13.4C1 10.904 11.297 2.502 24 2.502s23 8.402 23 18.766z\" stroke-width=\"2\" stroke-linejoin=\"round\"></path><path d=\"M25 28.502a2 2 0 1 0 0 4 2 2 0 0 0 0-4\" fill=\"#231F1F\"></path><path d=\"M19 17.75c0-3.312 2.686-6.124 6-6.124 3.313 0 6 2.626 6 5.938 0 3.315-2.687 5.938-6 5.938V26\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Frequently Asked Questions (FAQ)</h2>\n",
      "            <p class=\"paper__preview\">Common questions that arise during content creation.</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2366194/square_128/richie-in-hairnet-1537451295.JPG?1537451295\" alt=\"Richie Cotton avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2678765/square_128/peterson-1539265042.jpg?1539265042\" alt=\"Amy Peterson avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <span class=\"avatar__image avatar__fallback\"> B </span>\n",
      "\n",
      "      <span class=\"avatar__image avatar__fallback\">+2</span>\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        43 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Richie Cotton,</span> <span class='c__darker'> Amy Peterson,</span> <span class='c__darker'> Boris Gordts</span> and 2 others\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "    <div class=\"g__space\">\n",
      "      <a href=\"/en/collections/1340920-miscellaneous\" class=\"paper \">\n",
      "        <div class=\"collection o__ltr\">\n",
      "          <div class=\"collection__photo\">\n",
      "            <svg role='img' viewBox='0 0 48 48'><g id=\"tools-edit\"><path d=\"M14.932 43.968L2 47l3.033-12.93 31.2-31.203a4 4 0 0 1 5.658 0l4.247 4.243a4 4 0 0 1 0 5.656L14.932 43.968zm29.84-29.735L34.82 4.28m7.125 12.782L31.992 7.11M15.436 43.465l-9.9-9.9\" stroke-width=\"2\" fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></g></svg>\n",
      "          </div>\n",
      "          <div class=\"collection_meta\" dir=\"ltr\">\n",
      "            <h2 class=\"t__h3 c__primary\">Miscellaneous</h2>\n",
      "            <p class=\"paper__preview\">Have a question for DataCamp, but not about creating content? You&#39;ll probably find the answer here.</p>\n",
      "            <div class=\"avatar\">\n",
      "  <div class=\"avatar__photo avatars__images o__ltr\">\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2352718/square_128/Rebecca_Robins_-_Headshot-1535969735.jpg?1535969735\" alt=\"Becca Robins avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2830289/square_128/IMG_0665_a-1545331304.jpg?1545331304\" alt=\"Lisa Monteleone avatar\" class=\"avatar__image\">\n",
      "\n",
      "        <img src=\"https://static.intercomassets.com/avatars/2859053/square_128/gabriel_about_pic-1546620603.jpg?1546620603\" alt=\"Gabriel de Selding avatar\" class=\"avatar__image\">\n",
      "\n",
      "  </div>\n",
      "  <div class=\"avatar__info\">\n",
      "    <div>\n",
      "      <span class=\"c__darker\">\n",
      "        9 articles in this collection\n",
      "      </span>\n",
      "      <br>\n",
      "      Written by <span class='c__darker'> Becca Robins,</span> <span class='c__darker'> Lisa Monteleone,</span> and <span class='c__darker'> Gabriel de Selding</span>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </a>\n",
      "    </div>\n",
      "</section>\n",
      "</div>\n",
      "    </div>\n",
      "    <footer class=\"footer\">\n",
      "  <div class=\"container\">\n",
      "    <div class=\"content\">\n",
      "      <div class=\"u__cf\"  dir=\"ltr\">\n",
      "        <div class=\"footer__logo\">\n",
      "          <a href=\"/en/\">\n",
      "              <img alt=\"DataCamp Help Center\" src=\"https://downloads.intercomcdn.com/i/o/81221/856b63d438031754b681746b/4ea2737e4266936fb423911d9c587812.png\" />\n",
      "          </a>\n",
      "        </div>\n",
      "          <div class=\"footer__advert logo\">\n",
      "            <img src=\"https://static.intercomassets.com/alexandria/assets/intercom-a6a6ac0f033657af1aebe2e9e15b94a3cd5eabf6ae8b9916df6ea49099a894d8.png\" alt=\"Intercom\" />\n",
      "            <a href=\"https://www.intercom.com/intercom-link?company=DataCamp&amp;solution=customer-support&amp;utm_campaign=intercom-link&amp;utm_content=We+run+on+Intercom&amp;utm_medium=help-center&amp;utm_referrer=http%3A%2F%2Finstructor-support.datacamp.com%2Fen&amp;utm_source=desktop-web\">We run on Intercom</a>\n",
      "          </div>\n",
      "      </div>\n",
      "    </div>\n",
      "  </div>\n",
      "</footer>\n",
      "\n",
      "    \n",
      "  <script nonce=\"jc63n4PV4I/DWFe5pwm4DvUthDBy5Ltpoea8uJqSLoM=\">\n",
      "    window.intercomSettings = {\"app_id\":\"ug0ps1rq\"};\n",
      "</script>\n",
      "  <script nonce=\"jc63n4PV4I/DWFe5pwm4DvUthDBy5Ltpoea8uJqSLoM=\">\n",
      "    (function(){var w=window;var ic=w.Intercom;if(typeof ic===\"function\"){ic('reattach_activator');ic('update',intercomSettings);}else{var d=document;var i=function(){i.c(arguments)};i.q=[];i.c=function(args){i.q.push(args)};w.Intercom=i;function l(){var s=d.createElement('script');s.type='text/javascript';s.async=true;s.src=\"https://widget.intercom.io/widget/ug0ps1rq\";var x=d.getElementsByTagName('script')[0];x.parentNode.insertBefore(s,x);}if(w.attachEvent){w.attachEvent('onload',l);}else{w.addEventListener('load',l,false);}}})()\n",
      "</script>\n",
      "\n",
      "    \n",
      "\n",
      "    <script src=\"https://static.intercomassets.com/alexandria/assets/application-3fdd0e042b1b4fca26c3077843f03e9259080c7fc4d61584f7abe65f49e70283.js\" nonce=\"jc63n4PV4I/DWFe5pwm4DvUthDBy5Ltpoea8uJqSLoM=\"></script>\n",
      "    \n",
      "  </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = 'http://www.datacamp.com/teach/documentation'\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the Web in Python\n",
    "\n",
    "HTML is a mix of unstructured and structured data. Structured data has a pre-defined data model or is organized in a defined manner. Unstructured data has neither of these qualities. The unstructured data does have tags that can be used to determine where headings and hyperlinks can be found. To turn HTML from the world wide web into useful data, you need to parse it and extract structured data from it. The Python package BeautifulSoup can help you parse html. In coding, tag soup is a term to refer to structurally incorrect HTML written for a web page. The BeautifulSoup package can help with parsing tag soup HTML with its prettify() function. When you print the prettified HTML, you see that it is indented in the way you expect properly written HTML code to be. \n",
    "\n",
    "BeautifulSoup has methods like titls and get_text() that can be used to get the HTML page's title and text respectively. And functions like find_all() that finds all of the hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n",
      "\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <title>\n",
      "   Beautiful Soup: We called him Tortoise because he taught us.\n",
      "  </title>\n",
      "  <link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n",
      "  <link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n",
      "  <meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n",
      "  <meta content=\"Leonard Richardson\" name=\"author\"/>\n",
      " </head>\n",
      " <body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n",
      "  <style>\n",
      "   #tidelift { }\n",
      "\n",
      "#tidelift a {\n",
      " border: 1px solid #666666;\n",
      " margin-left: auto;\n",
      " padding: 10px;\n",
      " text-decoration: none;\n",
      "}\n",
      "\n",
      "#tidelift .cta {\n",
      " background: url(\"tidelift.svg\") no-repeat;\n",
      " padding-left: 30px;\n",
      "}\n",
      "  </style>\n",
      "  <img align=\"right\" src=\"10.1.jpg\" width=\"250\"/>\n",
      "  <br/>\n",
      "  <p>\n",
      "   [\n",
      "   <a href=\"#Download\">\n",
      "    Download\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"bs4/doc/\">\n",
      "    Documentation\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"#HallOfFame\">\n",
      "    Hall of Fame\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"enterprise.html\">\n",
      "    For enterprise\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"https://code.launchpad.net/beautifulsoup\">\n",
      "    Source\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">\n",
      "    Changelog\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "    Discussion group\n",
      "   </a>\n",
      "   |\n",
      "   <a href=\"zine/\">\n",
      "    Zine\n",
      "   </a>\n",
      "   ]\n",
      "  </p>\n",
      "  <div align=\"center\">\n",
      "   <a href=\"bs4/download/\">\n",
      "    <h1>\n",
      "     Beautiful Soup\n",
      "    </h1>\n",
      "   </a>\n",
      "  </div>\n",
      "  <p>\n",
      "   You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "   <ol>\n",
      "    <li>\n",
      "     Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "     <li>\n",
      "      Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "      <li>\n",
      "       Beautiful Soup sits on top of popular Python parsers like\n",
      "       <a href=\"http://lxml.de/\">\n",
      "        lxml\n",
      "       </a>\n",
      "       and\n",
      "       <a href=\"http://code.google.com/p/html5lib/\">\n",
      "        html5lib\n",
      "       </a>\n",
      "       , allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "      </li>\n",
      "     </li>\n",
      "    </li>\n",
      "   </ol>\n",
      "   <p>\n",
      "    Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class\n",
      "    <tt>\n",
      "     externalLink\n",
      "    </tt>\n",
      "    \", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "    <p>\n",
      "     Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "     <p>\n",
      "      Interested?\n",
      "      <a href=\"bs4/doc/\">\n",
      "       Read more.\n",
      "      </a>\n",
      "      <h3>\n",
      "       Getting and giving support\n",
      "      </h3>\n",
      "      <div align=\"center\" id=\"tidelift\">\n",
      "       <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=enterprise\" target=\"_blank\">\n",
      "        <span class=\"cta\">\n",
      "         Beautiful Soup for enterprise available via Tidelift\n",
      "        </span>\n",
      "       </a>\n",
      "      </div>\n",
      "      <p>\n",
      "       If you have questions, send them to\n",
      "       <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "        the discussion\n",
      "group\n",
      "       </a>\n",
      "       . If you find a bug,\n",
      "       <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "        file it on Launchpad\n",
      "       </a>\n",
      "       . If it's a security vulnerability, report it confidentially through\n",
      "       <a href=\"https://tidelift.com/security\">\n",
      "        Tidelift\n",
      "       </a>\n",
      "       .\n",
      "      </p>\n",
      "      <p>\n",
      "       If you use Beautiful Soup as part of your work, please consider a\n",
      "       <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">\n",
      "        Tidelift subscription\n",
      "       </a>\n",
      "       . This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "       <p>\n",
      "        If Beautiful Soup is useful to you on a personal level, you might like to read\n",
      "        <a href=\"zine/\">\n",
      "         <i>\n",
      "          Tool Safety\n",
      "         </i>\n",
      "        </a>\n",
      "        , a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "       </p>\n",
      "      </p>\n",
      "     </p>\n",
      "    </p>\n",
      "   </p>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "<a name=\"Download\">\n",
      " <h2>\n",
      "  Download Beautiful Soup\n",
      " </h2>\n",
      "</a>\n",
      "<p>\n",
      " The current release is\n",
      " <a href=\"bs4/download/\">\n",
      "  Beautiful Soup\n",
      "4.9.1\n",
      " </a>\n",
      " (May 17, 2020). You can install Beautiful Soup 4 with\n",
      " <code>\n",
      "  pip install beautifulsoup4\n",
      " </code>\n",
      " .\n",
      " <p>\n",
      "  In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "  <code>\n",
      "   python-bs4\n",
      "  </code>\n",
      "  package (for Python 2) or the\n",
      "  <code>\n",
      "   python3-bs4\n",
      "  </code>\n",
      "  package (for Python 3). In Fedora it's\n",
      "available as the\n",
      "  <code>\n",
      "   python-beautifulsoup4\n",
      "  </code>\n",
      "  package.\n",
      "  <p>\n",
      "   Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the\n",
      "   <code>\n",
      "    bs4/\n",
      "   </code>\n",
      "   directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using\n",
      "   <code>\n",
      "    2to3\n",
      "   </code>\n",
      "   .)\n",
      "   <p>\n",
      "    Beautiful Soup 4 works on both Python 2 (2.7+) and Python\n",
      "3. Support for Python 2 will be discontinued on or after December 31,\n",
      "2020—one year after the Python 2 sunsetting date.\n",
      "    <h3>\n",
      "     Beautiful Soup 3\n",
      "    </h3>\n",
      "    <p>\n",
      "     Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It does not support Python 3 and it will\n",
      "be discontinued on or after December 31, 2020—one year after the\n",
      "Python 2 sunsetting date. If you have any active projects using\n",
      "Beautiful Soup 3, you should migrate to Beautiful Soup 4 as part of\n",
      "your Python 3 conversion.\n",
      "     <p>\n",
      "      <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">\n",
      "       Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "      </a>\n",
      "      <p>\n",
      "       The current and hopefully final release of Beautiful Soup 3 is\n",
      "       <a href=\"download/3.x/BeautifulSoup-3.2.2.tar.gz\">\n",
      "        3.2.2\n",
      "       </a>\n",
      "       (October 5,\n",
      "2019). It's the\n",
      "       <code>\n",
      "        BeautifulSoup\n",
      "       </code>\n",
      "       package on pip. It's also\n",
      "available as\n",
      "       <code>\n",
      "        python-beautifulsoup\n",
      "       </code>\n",
      "       in Debian and Ubuntu,\n",
      "and as\n",
      "       <code>\n",
      "        python-BeautifulSoup\n",
      "       </code>\n",
      "       in Fedora.\n",
      "       <p>\n",
      "        Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n",
      "        <p>\n",
      "         Beautiful Soup 3, like Beautiful Soup 4, is\n",
      "         <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&amp;utm_medium=referral&amp;utm_campaign=website\">\n",
      "          supported through Tidelift\n",
      "         </a>\n",
      "         .\n",
      "        </p>\n",
      "        <a name=\"HallOfFame\">\n",
      "         <h2>\n",
      "          Hall of Fame\n",
      "         </h2>\n",
      "        </a>\n",
      "        <p>\n",
      "         Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "         <ul>\n",
      "          <li>\n",
      "           <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\n",
      "            \"Movable\n",
      " Type\"\n",
      "           </a>\n",
      "           , a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "           <li>\n",
      "            Jiabao Lin's\n",
      "            <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">\n",
      "             DXY-COVID-19-Crawler\n",
      "            </a>\n",
      "            uses Beautiful Soup to scrape a Chinese medical site for information\n",
      "about COVID-19, making it easier for researchers to track the spread\n",
      "of the virus. (Source:\n",
      "            <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\n",
      "             \"How open source software is fighting COVID-19\"\n",
      "            </a>\n",
      "            )\n",
      "            <li>\n",
      "             Reddit uses Beautiful Soup to\n",
      "             <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">\n",
      "              parse\n",
      "a page that's been linked to and find a representative image\n",
      "             </a>\n",
      "             .\n",
      "             <li>\n",
      "              Alexander Harrowell uses Beautiful Soup to\n",
      "              <a href=\"http://www.harrowell.org.uk/viktormap.html\">\n",
      "               track the business\n",
      " activities\n",
      "              </a>\n",
      "              of an arms merchant.\n",
      "              <li>\n",
      "               The developers of Python itself used Beautiful Soup to\n",
      "               <a href=\"http://svn.python.org/view/tracker/importer/\">\n",
      "                migrate the Python\n",
      "bug tracker from Sourceforge to Roundup\n",
      "               </a>\n",
      "               .\n",
      "               <li>\n",
      "                The\n",
      "                <a href=\"http://www2.ljworld.com/\">\n",
      "                 Lawrence Journal-World\n",
      "                </a>\n",
      "                uses Beautiful Soup to\n",
      "                <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">\n",
      "                 gather\n",
      "statewide election results\n",
      "                </a>\n",
      "                .\n",
      "                <li>\n",
      "                 The\n",
      "                 <a href=\"http://esrl.noaa.gov/gsd/fab/\">\n",
      "                  NOAA's Forecast\n",
      "Applications Branch\n",
      "                 </a>\n",
      "                 uses Beautiful Soup in\n",
      "                 <a href=\"http://laps.noaa.gov/topograbber/\">\n",
      "                  TopoGrabber\n",
      "                 </a>\n",
      "                 , a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "                </li>\n",
      "               </li>\n",
      "              </li>\n",
      "             </li>\n",
      "            </li>\n",
      "           </li>\n",
      "          </li>\n",
      "         </ul>\n",
      "         <p>\n",
      "          If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or\n",
      "          <a href=\"http://groups.google.com/group/beautifulsoup/\">\n",
      "           the discussion\n",
      "group\n",
      "          </a>\n",
      "          .\n",
      "          <h2>\n",
      "           Development\n",
      "          </h2>\n",
      "          <p>\n",
      "           Development happens at\n",
      "           <a href=\"https://launchpad.net/beautifulsoup\">\n",
      "            Launchpad\n",
      "           </a>\n",
      "           . You can\n",
      "           <a href=\"https://code.launchpad.net/beautifulsoup/\">\n",
      "            get the source\n",
      "code\n",
      "           </a>\n",
      "           or\n",
      "           <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "            file\n",
      "bugs\n",
      "           </a>\n",
      "           .\n",
      "           <hr/>\n",
      "           <table>\n",
      "            <tr>\n",
      "             <td valign=\"top\">\n",
      "              <p>\n",
      "               This document (\n",
      "               <a href=\"/source/software/BeautifulSoup/index.bhtml\">\n",
      "                source\n",
      "               </a>\n",
      "               ) is part of Crummy, the webspace of\n",
      "               <a href=\"/self/\">\n",
      "                Leonard Richardson\n",
      "               </a>\n",
      "               (\n",
      "               <a href=\"/self/contact.html\">\n",
      "                contact information\n",
      "               </a>\n",
      "               ). It was last modified on Sunday, May 17 2020, 18:46:28 Nowhere Standard Time and last built on Thursday, August 27 2020, 18:00:01 Nowhere Standard Time.\n",
      "              </p>\n",
      "              <p>\n",
      "               <table class=\"licenseText\">\n",
      "                <tr>\n",
      "                 <td>\n",
      "                  <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "                   <img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/>\n",
      "                  </a>\n",
      "                 </td>\n",
      "                 <td valign=\"top\">\n",
      "                  Crummy is © 1996-2020 Leonard Richardson. Unless otherwise noted, all text licensed under a\n",
      "                  <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "                   Creative Commons License\n",
      "                  </a>\n",
      "                  .\n",
      "                 </td>\n",
      "                </tr>\n",
      "               </table>\n",
      "              </p>\n",
      "             </td>\n",
      "            </tr>\n",
      "           </table>\n",
      "          </p>\n",
      "         </p>\n",
      "        </p>\n",
      "       </p>\n",
      "      </p>\n",
      "     </p>\n",
      "    </p>\n",
      "   </p>\n",
      "  </p>\n",
      " </p>\n",
      "</p>\n",
      "<!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>-->\n",
      "<td valign=\"top\">\n",
      " <p>\n",
      "  <b>\n",
      "   Document tree:\n",
      "  </b>\n",
      "  <dl>\n",
      "   <dd>\n",
      "    <a href=\"http://www.crummy.com/\">\n",
      "     http://www.crummy.com/\n",
      "    </a>\n",
      "    <dl>\n",
      "     <dd>\n",
      "      <a href=\"http://www.crummy.com/software/\">\n",
      "       software/\n",
      "      </a>\n",
      "      <dl>\n",
      "       <dd>\n",
      "        <a href=\"http://www.crummy.com/software/BeautifulSoup/\">\n",
      "         BeautifulSoup/\n",
      "        </a>\n",
      "       </dd>\n",
      "      </dl>\n",
      "     </dd>\n",
      "    </dl>\n",
      "   </dd>\n",
      "  </dl>\n",
      "  Site Search:\n",
      "  <form action=\"/search/\" method=\"get\">\n",
      "   <input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n",
      "  </form>\n",
      " </p>\n",
      "</td>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "url = 'https://www.crummy.com/software/BeautifulSoup'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Beautiful Soup: We called him Tortoise because he taught us.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[ Download | Documentation | Hall of Fame | For enterprise | Source | Changelog | Discussion group  | Zine ]\n",
      "\n",
      "Beautiful Soup\n",
      "\n",
      "You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "\n",
      "\n",
      "Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "\n",
      "Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "\n",
      "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "\n",
      "\n",
      "Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class externalLink\", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "\n",
      "Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "\n",
      "Interested? Read more.\n",
      "Getting and giving support\n",
      "\n",
      "\n",
      "\n",
      "  Beautiful Soup for enterprise available via Tidelift\n",
      " \n",
      "\n",
      "\n",
      "If you have questions, send them to the discussion\n",
      "group. If you find a bug, file it on Launchpad. If it's a security vulnerability, report it confidentially through Tidelift.\n",
      "If you use Beautiful Soup as part of your work, please consider a Tidelift subscription. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "\n",
      "\n",
      "If Beautiful Soup is useful to you on a personal level, you might like to read Tool Safety, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "\n",
      "Download Beautiful Soup\n",
      "The current release is Beautiful Soup\n",
      "4.9.1 (May 17, 2020). You can install Beautiful Soup 4 with\n",
      "pip install beautifulsoup4.\n",
      "\n",
      "In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "python-bs4 package (for Python 2) or the\n",
      "python3-bs4 package (for Python 3). In Fedora it's\n",
      "available as the python-beautifulsoup4 package.\n",
      "\n",
      "Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the bs4/ directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using 2to3.)\n",
      "\n",
      "Beautiful Soup 4 works on both Python 2 (2.7+) and Python\n",
      "3. Support for Python 2 will be discontinued on or after December 31,\n",
      "2020—one year after the Python 2 sunsetting date.\n",
      "\n",
      "Beautiful Soup 3\n",
      "Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It does not support Python 3 and it will\n",
      "be discontinued on or after December 31, 2020—one year after the\n",
      "Python 2 sunsetting date. If you have any active projects using\n",
      "Beautiful Soup 3, you should migrate to Beautiful Soup 4 as part of\n",
      "your Python 3 conversion.\n",
      "\n",
      "Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "The current and hopefully final release of Beautiful Soup 3 is 3.2.2 (October 5,\n",
      "2019). It's the BeautifulSoup package on pip. It's also\n",
      "available as python-beautifulsoup in Debian and Ubuntu,\n",
      "and as python-BeautifulSoup in Fedora.\n",
      "\n",
      "Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n",
      "\n",
      "Beautiful Soup 3, like Beautiful Soup 4, is supported through Tidelift.\n",
      "Hall of Fame\n",
      "Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "\n",
      "\n",
      "\"Movable\n",
      " Type\", a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "\n",
      "Jiabao Lin's DXY-COVID-19-Crawler\n",
      "uses Beautiful Soup to scrape a Chinese medical site for information\n",
      "about COVID-19, making it easier for researchers to track the spread\n",
      "of the virus. (Source: \"How open source software is fighting COVID-19\")\n",
      "\n",
      "Reddit uses Beautiful Soup to parse\n",
      "a page that's been linked to and find a representative image.\n",
      "\n",
      "Alexander Harrowell uses Beautiful Soup to track the business\n",
      " activities of an arms merchant.\n",
      "\n",
      "The developers of Python itself used Beautiful Soup to migrate the Python\n",
      "bug tracker from Sourceforge to Roundup.\n",
      "\n",
      "The Lawrence Journal-World\n",
      "uses Beautiful Soup to gather\n",
      "statewide election results.\n",
      "\n",
      "The NOAA's Forecast\n",
      "Applications Branch uses Beautiful Soup in TopoGrabber, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "\n",
      "If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or the discussion\n",
      "group.\n",
      "\n",
      "Development\n",
      "Development happens at Launchpad. You can get the source\n",
      "code or file\n",
      "bugs.\n",
      "This document (source) is part of Crummy, the webspace of Leonard Richardson (contact information). It was last modified on Sunday, May 17 2020, 18:46:28 Nowhere Standard Time and last built on Thursday, August 27 2020, 18:00:01 Nowhere Standard Time.Crummy is © 1996-2020 Leonard Richardson. Unless otherwise noted, all text licensed under a Creative Commons License.Document tree:\n",
      "http://www.crummy.com/software/BeautifulSoup/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Site Search:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Download\n",
      "bs4/doc/\n",
      "#HallOfFame\n",
      "enterprise.html\n",
      "https://code.launchpad.net/beautifulsoup\n",
      "https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "zine/\n",
      "bs4/download/\n",
      "http://lxml.de/\n",
      "http://code.google.com/p/html5lib/\n",
      "bs4/doc/\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise\n",
      "https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "https://tidelift.com/security\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\n",
      "zine/\n",
      "None\n",
      "bs4/download/\n",
      "http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\n",
      "download/3.x/BeautifulSoup-3.2.2.tar.gz\n",
      "https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website\n",
      "None\n",
      "http://www.nytimes.com/2007/10/25/arts/design/25vide.html\n",
      "https://github.com/BlankerL/DXY-COVID-19-Crawler\n",
      "https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\n",
      "https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\n",
      "http://www.harrowell.org.uk/viktormap.html\n",
      "http://svn.python.org/view/tracker/importer/\n",
      "http://www2.ljworld.com/\n",
      "http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\n",
      "http://esrl.noaa.gov/gsd/fab/\n",
      "http://laps.noaa.gov/topograbber/\n",
      "http://groups.google.com/group/beautifulsoup/\n",
      "https://launchpad.net/beautifulsoup\n",
      "https://code.launchpad.net/beautifulsoup/\n",
      "https://bugs.launchpad.net/beautifulsoup/\n",
      "/source/software/BeautifulSoup/index.bhtml\n",
      "/self/\n",
      "/self/contact.html\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://creativecommons.org/licenses/by-sa/2.0/\n",
      "http://www.crummy.com/\n",
      "http://www.crummy.com/software/\n",
      "http://www.crummy.com/software/BeautifulSoup/\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "#### Parsing HTML with BeautifulSoup\n",
    "In this interactive exercise, you'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own Benevolent Dictator for Life. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.\n",
    "\n",
    "The URL of interest is url = 'https://www.python.org/~guido/'.\n",
    "\n",
    "__Instructions:__\n",
    "* Import the function BeautifulSoup from the package bs4.\n",
    "* Assign the URL of interest to the variable url.\n",
    "* Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.\n",
    "* Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable html_doc.\n",
    "* Create a BeautifulSoup object soup from the resulting HTML using the function BeautifulSoup().\n",
    "* Use the method prettify() on soup and assign the result to pretty_soup.\n",
    "* Hit submit to print to prettified HTML to your shell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" height=\"216\" src=\"images/guido-headshot-2019.jpg\" width=\"270\"/>\n",
      "   </a>\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "   <h3>\n",
      "    <a href=\"images/df20000406.jpg\">\n",
      "     Who I Am\n",
      "    </a>\n",
      "   </h3>\n",
      "   <p>\n",
      "    Read\n",
      "my\n",
      "    <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "     \"King's\n",
      "Day Speech\"\n",
      "    </a>\n",
      "    for some inspiration.\n",
      "    <p>\n",
      "     I am the author of the\n",
      "     <a href=\"http://www.python.org\">\n",
      "      Python\n",
      "     </a>\n",
      "     programming language.  See also my\n",
      "     <a href=\"Resume.html\">\n",
      "      resume\n",
      "     </a>\n",
      "     and my\n",
      "     <a href=\"Publications.html\">\n",
      "      publications list\n",
      "     </a>\n",
      "     , a\n",
      "     <a href=\"bio.html\">\n",
      "      brief bio\n",
      "     </a>\n",
      "     , assorted\n",
      "     <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "      writings\n",
      "     </a>\n",
      "     ,\n",
      "     <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "      presentations\n",
      "     </a>\n",
      "     and\n",
      "     <a href=\"interviews.html\">\n",
      "      interviews\n",
      "     </a>\n",
      "     (all about Python), some\n",
      "     <a href=\"pics.html\">\n",
      "      pictures of me\n",
      "     </a>\n",
      "     ,\n",
      "     <a href=\"http://neopythonic.blogspot.com\">\n",
      "      my new blog\n",
      "     </a>\n",
      "     , and\n",
      "my\n",
      "     <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "      old\n",
      "blog\n",
      "     </a>\n",
      "     on Artima.com.  I am\n",
      "     <a href=\"https://twitter.com/gvanrossum\">\n",
      "      @gvanrossum\n",
      "     </a>\n",
      "     on Twitter.\n",
      "     <p>\n",
      "      I am retired, working on personal projects (and maybe a book).\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "      <a href=\"Resume.html\">\n",
      "       resume\n",
      "      </a>\n",
      "      .)  I created Python while at CWI.\n",
      "      <h3>\n",
      "       How to Reach Me\n",
      "      </h3>\n",
      "      <p>\n",
      "       You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "       <h3>\n",
      "        My Name\n",
      "       </h3>\n",
      "       <p>\n",
      "        My name often poses difficulties for Americans.\n",
      "        <p>\n",
      "         <b>\n",
      "          Pronunciation:\n",
      "         </b>\n",
      "         in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "         <a href=\"guido.au\">\n",
      "          sound clip\n",
      "         </a>\n",
      "         .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "         <p>\n",
      "          <b>\n",
      "           Spelling:\n",
      "          </b>\n",
      "          my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "          <p>\n",
      "           <b>\n",
      "            Alphabetization:\n",
      "           </b>\n",
      "           in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "           <h3>\n",
      "            More Hyperlinks\n",
      "           </h3>\n",
      "           <ul>\n",
      "            <li>\n",
      "             Here's a collection of\n",
      "             <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "              essays\n",
      "             </a>\n",
      "             relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "             <p>\n",
      "              <li>\n",
      "               I own the official\n",
      "               <a href=\"images/license.jpg\">\n",
      "                <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "                Python license.\n",
      "               </a>\n",
      "               <p>\n",
      "               </p>\n",
      "              </li>\n",
      "             </p>\n",
      "            </li>\n",
      "           </ul>\n",
      "           <h3>\n",
      "            The Audio File Formats FAQ\n",
      "           </h3>\n",
      "           <p>\n",
      "            I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at\n",
      "            <a href=\"http://www.cnpbagwell.com/audio-faq\">\n",
      "             http://www.cnpbagwell.com/audio-faq\n",
      "            </a>\n",
      "            .  And here is a link to\n",
      "            <a href=\"http://sox.sourceforge.net/\">\n",
      "             SOX\n",
      "            </a>\n",
      "            , to which I contributed\n",
      "some early code.\n",
      "           </p>\n",
      "          </p>\n",
      "         </p>\n",
      "        </p>\n",
      "       </p>\n",
      "      </p>\n",
      "     </p>\n",
      "    </p>\n",
      "   </p>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "<hr/>\n",
      "<a href=\"images/internetdog.gif\">\n",
      " \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "</a>\n",
      "<hr/>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning a webpage into data using BeautifulSoup: getting the text\n",
    "As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title.\n",
    "\n",
    "__Instructions:__\n",
    "\n",
    "* In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function BeautifulSoup() and to assign the resulting soup to the variable soup.\n",
    "* Extract the title from the HTML soup soup using the attribute title and assign the result to guido_title.\n",
    "* Print the title of Guido's webpage to the shell using the print() function.\n",
    "* Extract the text from the HTML soup soup using the method get_text() and assign to guido_text.\n",
    "* Hit submit to print the text from Guido's webpage to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "\n",
      "\n",
      "Guido's Personal Home Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Guido van Rossum - Personal Home Page\n",
      "\n",
      "\n",
      "\"Gawky and proud of it.\"\n",
      "Who I Am\n",
      "Read\n",
      "my \"King's\n",
      "Day Speech\" for some inspiration.\n",
      "\n",
      "I am the author of the Python\n",
      "programming language.  See also my resume\n",
      "and my publications list, a brief bio, assorted writings, presentations and interviews (all about Python), some\n",
      "pictures of me,\n",
      "my new blog, and\n",
      "my old\n",
      "blog on Artima.com.  I am\n",
      "@gvanrossum on Twitter.\n",
      "\n",
      "I am retired, working on personal projects (and maybe a book).\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my resume.)  I created Python while at CWI.\n",
      "\n",
      "How to Reach Me\n",
      "You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "\n",
      "My Name\n",
      "My name often poses difficulties for Americans.\n",
      "\n",
      "Pronunciation: in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "sound clip.)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "\n",
      "Spelling: my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "\n",
      "Alphabetization: in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "\n",
      "\n",
      "More Hyperlinks\n",
      "\n",
      "Here's a collection of essays relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "I own the official \n",
      "Python license.\n",
      "\n",
      "The Audio File Formats FAQ\n",
      "I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at http://www.cnpbagwell.com/audio-faq.  And here is a link to\n",
      "SOX, to which I contributed\n",
      "some early code.\n",
      "\n",
      "\n",
      "\n",
      "\"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.text\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning a webpage into data using BeautifulSoup: getting the hyperlinks\n",
    "In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method find_all().\n",
    "\n",
    "__Instructions:__\n",
    "* Use the method findall() to find all hyperlinks in soup, remembering that hyperlinks are defined by the HTML tag a but passed to findall() without angle brackets; store the result in the variable a_tags.\n",
    "* The variable a_tags is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the hyperlinks; to do this, for every element link in a_tags, you want to print() link.get('href')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "images/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "Resume.html\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
